{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67ba64f2",
   "metadata": {},
   "source": [
    "# Neural Posterior Estimation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1813fb16",
   "metadata": {},
   "source": [
    "## Notes:\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "g8m30ri74nr",
   "source": "import torch\nimport torch.nn as nn\nimport torch.distributions as dist\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# =============================================================================\n# WHAT IS A NORMALIZING FLOW?\n# =============================================================================\n# A normalizing flow is a type of neural network that learns to transform \n# a simple distribution (like a Gaussian) into a complex distribution.\n# \n# The key idea: Start with noise z ~ N(0,1) and apply invertible transformations\n# to get x = f(z). Because the transformations are invertible, we can also go\n# backwards: z = f^(-1)(x)\n#\n# This lets us:\n# 1. Sample: Draw z from N(0,1), transform it to get samples from our target\n# 2. Compute density: Given x, find z = f^(-1)(x) and compute probability\n# =============================================================================\n\n\nclass AffineCouplingLayer(nn.Module):\n    \"\"\"\n    An Affine Coupling Layer - the building block of our normalizing flow.\n    \n    How it works:\n    1. Split input in half: x = [x_a, x_b]\n    2. Keep first half unchanged: y_a = x_a\n    3. Transform second half based on first half: y_b = x_b * exp(s(x_a)) + t(x_a)\n       where s() and t() are neural networks that output \"scale\" and \"translation\"\n    \n    Why this is clever:\n    - It's easily invertible (we can go backwards)\n    - It's expressive (neural networks can learn complex patterns)\n    - Computing the determinant of the Jacobian is easy (needed for density computation)\n    \"\"\"\n    \n    def __init__(self, dim, hidden_dim=64, mask=None):\n        super().__init__()\n        self.dim = dim\n        \n        # Mask determines which dimensions are transformed\n        # If mask=None, we split in half (first half fixed, second half transformed)\n        if mask is None:\n            mask = torch.zeros(dim)\n            mask[dim // 2:] = 1\n        self.register_buffer('mask', mask)\n        \n        # Count how many dimensions are being transformed\n        num_transform = int(mask.sum().item())\n        num_condition = dim - num_transform\n        \n        # Neural network that outputs scale (s) and translation (t) parameters\n        # Input: conditioning dimensions, Output: scale and translation for transformed dimensions\n        self.scale_translate_net = nn.Sequential(\n            nn.Linear(num_condition, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, num_transform * 2)  # Output both scale and translate\n        )\n        \n    def forward(self, x):\n        \"\"\"\n        Transform x -> y (from simple to complex distribution)\n        Returns: (y, log_det) where log_det is log determinant of Jacobian\n        \"\"\"\n        # Split into conditioning part (masked=0) and transform part (masked=1)\n        x_condition = x * (1 - self.mask)  # Parts that stay fixed\n        x_transform = x * self.mask  # Parts that get transformed\n        \n        # Get the actual values (non-zero parts)\n        x_cond_vals = x[:, self.mask == 0]\n        \n        # Use conditioning part to compute scale and translation\n        st = self.scale_translate_net(x_cond_vals)\n        num_transform = int(self.mask.sum().item())\n        scale = st[:, :num_transform]\n        translate = st[:, num_transform:]\n        \n        # Transform the masked part: affine transformation\n        # We use tanh to keep scale bounded (prevents numerical instability)\n        scale = torch.tanh(scale)\n        \n        # Extract values to transform, apply transformation, put back\n        x_trans_vals = x[:, self.mask == 1]\n        y_trans_vals = x_trans_vals * torch.exp(scale) + translate\n        \n        # Reconstruct output\n        y = x_condition.clone()\n        y[:, self.mask == 1] = y_trans_vals\n        \n        # Log determinant of Jacobian (needed for computing probability)\n        # For affine coupling, this is just the sum of the log scales\n        log_det = scale.sum(dim=1)\n        \n        return y, log_det\n    \n    def inverse(self, y):\n        \"\"\"\n        Transform y -> x (from complex back to simple distribution)\n        This is the inverse of forward()\n        \"\"\"\n        # Split into conditioning and transform parts\n        y_condition = y * (1 - self.mask)\n        \n        # Get conditioning values\n        y_cond_vals = y[:, self.mask == 0]\n        \n        # Compute scale and translation (same as forward)\n        st = self.scale_translate_net(y_cond_vals)\n        num_transform = int(self.mask.sum().item())\n        scale = st[:, :num_transform]\n        translate = st[:, num_transform:]\n        scale = torch.tanh(scale)\n        \n        # Invert the affine transformation\n        y_trans_vals = y[:, self.mask == 1]\n        x_trans_vals = (y_trans_vals - translate) * torch.exp(-scale)\n        \n        # Reconstruct input\n        x = y_condition.clone()\n        x[:, self.mask == 1] = x_trans_vals\n        \n        return x\n\n\nclass NormalizingFlow(nn.Module):\n    \"\"\"\n    A simple Normalizing Flow: stack multiple coupling layers together.\n    \n    Each layer transforms the data a bit more, and stacking them allows\n    the model to learn very complex transformations.\n    \n    IMPORTANT: We alternate the mask pattern so different dimensions get\n    transformed in each layer. This allows all dimensions to be affected!\n    \"\"\"\n    \n    def __init__(self, dim, num_layers=6, hidden_dim=128):\n        super().__init__()\n        self.dim = dim\n        \n        # Create alternating masks\n        # Layer 0: mask [0, 1] - transform second dimension based on first\n        # Layer 1: mask [1, 0] - transform first dimension based on second\n        # This pattern repeats, allowing all dimensions to influence each other\n        self.layers = nn.ModuleList()\n        for i in range(num_layers):\n            # Alternate which half is masked\n            mask = torch.zeros(dim)\n            if i % 2 == 0:\n                mask[dim // 2:] = 1  # Transform second half\n            else:\n                mask[:dim // 2] = 1  # Transform first half\n            \n            self.layers.append(AffineCouplingLayer(dim, hidden_dim, mask))\n        \n        # Base distribution: simple Gaussian\n        # This is what we start from when sampling\n        self.base_dist = dist.Normal(torch.zeros(dim), torch.ones(dim))\n        \n    def forward(self, z):\n        \"\"\"\n        Transform z (from base distribution) to x (target distribution)\n        Used for sampling: z ~ N(0,1) -> x ~ p(x)\n        \"\"\"\n        x = z\n        log_det_sum = 0\n        \n        # Apply each layer sequentially\n        for layer in self.layers:\n            x, log_det = layer(x)\n            log_det_sum += log_det\n            \n        return x, log_det_sum\n    \n    def inverse(self, x):\n        \"\"\"\n        Transform x (target distribution) back to z (base distribution)\n        Used for computing probability: x -> z ~ N(0,1)\n        \"\"\"\n        z = x\n        \n        # Apply layers in reverse order\n        for layer in reversed(self.layers):\n            z = layer.inverse(z)\n            \n        return z\n    \n    def log_prob(self, x):\n        \"\"\"\n        Compute log probability of x under the learned distribution.\n        \n        This is the change of variables formula:\n        log p(x) = log p(z) - log |det J|\n        where z = f^(-1)(x) and J is the Jacobian of the transformation\n        \"\"\"\n        z = self.inverse(x)\n        \n        # Compute forward pass to get log determinant\n        _, log_det = self.forward(z)\n        \n        # Log probability in base distribution\n        log_prob_z = self.base_dist.log_prob(z).sum(dim=1)\n        \n        # Change of variables formula\n        return log_prob_z - log_det\n    \n    def sample(self, num_samples):\n        \"\"\"\n        Sample from the learned distribution.\n        \n        1. Sample z from base distribution N(0,1)\n        2. Transform through the flow to get x\n        \"\"\"\n        # Sample from base distribution\n        z = self.base_dist.sample((num_samples,))\n        \n        # Transform to target distribution\n        x, _ = self.forward(z)\n        \n        return x\n\n\n# =============================================================================\n# EXAMPLE: Learn to transform a Gaussian into a two-moons distribution\n# =============================================================================\n\ndef make_moons_dataset(n_samples=1000):\n    \"\"\"Create a simple two-moons dataset (two crescents)\"\"\"\n    n = n_samples // 2\n    \n    # First moon\n    theta1 = np.linspace(0, np.pi, n)\n    x1 = np.cos(theta1)\n    y1 = np.sin(theta1)\n    \n    # Second moon (offset and flipped)\n    theta2 = np.linspace(0, np.pi, n)\n    x2 = 1 - np.cos(theta2)\n    y2 = 0.5 - np.sin(theta2)\n    \n    # Combine and add noise\n    x = np.concatenate([x1, x2])\n    y = np.concatenate([y1, y2])\n    data = np.column_stack([x, y])\n    data += np.random.normal(0, 0.1, data.shape)\n    \n    return torch.tensor(data, dtype=torch.float32)\n\n\n# Create training data\nprint(\"Creating two-moons dataset...\")\ndata = make_moons_dataset(2000)\n\n# Create the normalizing flow\nprint(\"\\nInitializing normalizing flow...\")\nflow = NormalizingFlow(dim=2, num_layers=8, hidden_dim=128)\n\n# Optimizer\noptimizer = torch.optim.Adam(flow.parameters(), lr=5e-4)\n\n# Training loop\nprint(\"\\nTraining the flow...\")\nnum_epochs = 2000\nbatch_size = 256\n\nfor epoch in range(num_epochs):\n    # Sample a batch\n    idx = torch.randperm(len(data))[:batch_size]\n    batch = data[idx]\n    \n    # Compute negative log likelihood (our loss)\n    # We want to maximize log p(x), which is the same as minimizing -log p(x)\n    log_prob = flow.log_prob(batch)\n    loss = -log_prob.mean()\n    \n    # Backprop and update\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    if (epoch + 1) % 400 == 0:\n        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n\nprint(\"\\nTraining complete!\")\n\n# =============================================================================\n# VISUALIZE THE RESULTS\n# =============================================================================\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Plot 1: Original data\naxes[0].scatter(data[:, 0], data[:, 1], alpha=0.5, s=10)\naxes[0].set_title(\"Original Data (Two Moons)\")\naxes[0].set_xlabel(\"x\")\naxes[0].set_ylabel(\"y\")\naxes[0].set_xlim(-1.5, 2.5)\naxes[0].set_ylim(-1, 2)\n\n# Plot 2: Samples from the trained flow\nwith torch.no_grad():\n    samples = flow.sample(2000)\naxes[1].scatter(samples[:, 0], samples[:, 1], alpha=0.5, s=10, color='orange')\naxes[1].set_title(\"Samples from Trained Flow\")\naxes[1].set_xlabel(\"x\")\naxes[1].set_ylabel(\"y\")\naxes[1].set_xlim(-1.5, 2.5)\naxes[1].set_ylim(-1, 2)\n\n# Plot 3: Show the transformation from base distribution\n# Sample from N(0,1) and show intermediate transformations\nwith torch.no_grad():\n    z = flow.base_dist.sample((1000,))\n    axes[2].scatter(z[:, 0], z[:, 1], alpha=0.2, s=5, label='Base (Gaussian)', color='blue')\n    \n    # Apply transformations one by one\n    x = z\n    for i, layer in enumerate(flow.layers):\n        x, _ = layer(x)\n        if i == len(flow.layers) // 2 - 1:  # Show intermediate step\n            axes[2].scatter(x[:, 0], x[:, 1], alpha=0.2, s=5, \n                          label=f'After {i+1} layers', color='green')\n    \n    axes[2].scatter(x[:, 0], x[:, 1], alpha=0.3, s=5, \n                   label='Final (Two Moons)', color='orange')\n    \naxes[2].set_title(\"Transformation Process\")\naxes[2].set_xlabel(\"x\")\naxes[2].set_ylabel(\"y\")\naxes[2].legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"SUMMARY: What did we learn?\")\nprint(\"=\"*70)\nprint(\"1. We started with a simple Gaussian distribution N(0,1)\")\nprint(\"2. We applied a series of learnable invertible transformations\")\nprint(\"3. ALTERNATING MASKS are crucial: each layer transforms different dimensions\")\nprint(\"   - Layer 0: fixes x, transforms y based on x\")\nprint(\"   - Layer 1: fixes y, transforms x based on y\")\nprint(\"   - This pattern repeats, allowing complex interactions!\")\nprint(\"4. The flow learned to transform the Gaussian into a two-moons shape\")\nprint(\"5. We can now:\")\nprint(\"   - Sample from the complex distribution (just transform Gaussian samples)\")\nprint(\"   - Compute probabilities (invert the transformation and use Gaussian formula)\")\nprint(\"\\nThis is the foundation of normalizing flows used in NPE!\")\nprint(\"=\"*70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "w596l4yx6fa",
   "source": "# =============================================================================\n# 1D NORMALIZING FLOW EXAMPLE - Even Simpler!\n# =============================================================================\n# For 1D data, we can't use coupling layers (we need at least 2 dimensions to\n# split and condition on each other). Instead, we'll use a different approach:\n# spline-based transformations that are guaranteed to be invertible.\n#\n# We'll use a piecewise-linear transformation, which is simple and effective!\n# =============================================================================\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\nclass PiecewiseLinearTransform(nn.Module):\n    \"\"\"\n    A piecewise-linear transformation for 1D data.\n    \n    How it works:\n    - Divide the x-axis into bins (like a histogram)\n    - Learn the \"heights\" for each bin\n    - Connect these heights with straight lines\n    - This creates a flexible, learnable transformation\n    \n    Why it's good:\n    - Simple to understand (just connecting dots!)\n    - Guaranteed invertible (monotonic)\n    - Easy to compute derivatives (constant within each segment)\n    \"\"\"\n    \n    def __init__(self, num_bins=16, bound=5.0):\n        super().__init__()\n        self.num_bins = num_bins\n        self.bound = bound  # We'll model data in range [-bound, bound]\n        \n        # Learnable parameters: the \"heights\" at each bin boundary\n        # We'll use softmax to ensure they sum to 1 (forms a valid CDF)\n        self.unnormalized_heights = nn.Parameter(torch.randn(num_bins))\n        \n    def forward(self, x):\n        \"\"\"\n        Transform x -> y using our piecewise-linear function\n        \n        Returns: (y, log_det_jacobian)\n        \"\"\"\n        batch_size = x.shape[0]\n        \n        # Get the heights (these form a CDF - cumulative distribution function)\n        # Softmax ensures they're positive and sum to 1\n        heights = F.softmax(self.unnormalized_heights, dim=0)\n        \n        # Create the bin edges (evenly spaced from -bound to +bound)\n        bin_edges = torch.linspace(-self.bound, self.bound, self.num_bins + 1).to(x.device)\n        \n        # Cumulative heights give us the y-values at each bin edge\n        # This is the CDF: cumulative distribution function\n        cumulative_heights = torch.cumsum(heights, dim=0)\n        cumulative_heights = torch.cat([torch.zeros(1).to(x.device), cumulative_heights])\n        \n        # Scale CDF to match our output range\n        y_edges = cumulative_heights * 2 * self.bound - self.bound\n        \n        # For each input x, find which bin it falls into\n        # Then interpolate linearly within that bin\n        y = torch.zeros_like(x)\n        log_det = torch.zeros(batch_size).to(x.device)\n        \n        for i in range(batch_size):\n            xi = x[i, 0]\n            \n            # Handle out-of-bounds inputs (rare, but can happen during training)\n            if xi <= -self.bound:\n                y[i, 0] = y_edges[0]\n                log_det[i] = 0\n                continue\n            if xi >= self.bound:\n                y[i, 0] = y_edges[-1]\n                log_det[i] = 0\n                continue\n            \n            # Find which bin this x falls into\n            bin_idx = torch.searchsorted(bin_edges, xi) - 1\n            bin_idx = torch.clamp(bin_idx, 0, self.num_bins - 1)\n            \n            # Linear interpolation within the bin\n            # Like drawing a line between two dots!\n            x_left = bin_edges[bin_idx]\n            x_right = bin_edges[bin_idx + 1]\n            y_left = y_edges[bin_idx]\n            y_right = y_edges[bin_idx + 1]\n            \n            # How far are we through this bin? (0 = left edge, 1 = right edge)\n            alpha = (xi - x_left) / (x_right - x_left)\n            \n            # Interpolate to get y\n            y[i, 0] = y_left + alpha * (y_right - y_left)\n            \n            # The derivative is the slope of this line segment\n            # This is constant within each bin (that's why it's \"piecewise linear\")\n            slope = (y_right - y_left) / (x_right - x_left)\n            log_det[i] = torch.log(torch.abs(slope) + 1e-8)  # Add small epsilon for numerical stability\n        \n        return y, log_det\n    \n    def inverse(self, y):\n        \"\"\"\n        Transform y -> x (inverse operation)\n        \n        Since our function is piecewise linear, the inverse is also piecewise linear!\n        We just swap the roles of x and y.\n        \"\"\"\n        batch_size = y.shape[0]\n        \n        # Get heights and edges (same as forward)\n        heights = F.softmax(self.unnormalized_heights, dim=0)\n        bin_edges = torch.linspace(-self.bound, self.bound, self.num_bins + 1).to(y.device)\n        cumulative_heights = torch.cumsum(heights, dim=0)\n        cumulative_heights = torch.cat([torch.zeros(1).to(y.device), cumulative_heights])\n        y_edges = cumulative_heights * 2 * self.bound - self.bound\n        \n        x = torch.zeros_like(y)\n        \n        for i in range(batch_size):\n            yi = y[i, 0]\n            \n            # Handle out-of-bounds\n            if yi <= y_edges[0]:\n                x[i, 0] = -self.bound\n                continue\n            if yi >= y_edges[-1]:\n                x[i, 0] = self.bound\n                continue\n            \n            # Find which bin this y falls into\n            bin_idx = torch.searchsorted(y_edges, yi) - 1\n            bin_idx = torch.clamp(bin_idx, 0, self.num_bins - 1)\n            \n            # Inverse interpolation: solve for x given y\n            x_left = bin_edges[bin_idx]\n            x_right = bin_edges[bin_idx + 1]\n            y_left = y_edges[bin_idx]\n            y_right = y_edges[bin_idx + 1]\n            \n            # What's the fraction along the y-axis?\n            alpha = (yi - y_left) / (y_right - y_left + 1e-8)\n            \n            # Convert to x position\n            x[i, 0] = x_left + alpha * (x_right - x_left)\n        \n        return x\n\n\nclass NormalizingFlow1D(nn.Module):\n    \"\"\"\n    A 1D Normalizing Flow: stack multiple piecewise-linear transformations.\n    \n    Each layer learns a different warping of the space, and together they\n    can represent complex distributions!\n    \"\"\"\n    def __init__(self, num_layers=6, num_bins=32):\n        super().__init__()\n        \n        # Stack multiple transformation layers\n        # Each one learns a different piecewise-linear function\n        self.layers = nn.ModuleList([\n            PiecewiseLinearTransform(num_bins=num_bins, bound=5.0) \n            for _ in range(num_layers)\n        ])\n        \n        # Base distribution: standard Gaussian N(0, 1)\n        self.base_dist = torch.distributions.Normal(0, 1)\n    \n    def forward(self, z):\n        \"\"\"\n        Transform z ~ N(0,1) to x ~ p(x)\n        \n        This is what we use for SAMPLING:\n        1. Draw random z from Gaussian\n        2. Apply transformations to get x\n        \"\"\"\n        x = z\n        log_det_sum = 0\n        \n        # Apply each transformation in sequence\n        for layer in self.layers:\n            x, log_det = layer(x)\n            log_det_sum += log_det\n        \n        return x, log_det_sum\n    \n    def inverse(self, x):\n        \"\"\"\n        Transform x back to z ~ N(0,1)\n        \n        This is what we use for computing PROBABILITY:\n        1. Take data point x\n        2. Transform back to find what z would have produced it\n        3. Compute probability of that z under Gaussian\n        \"\"\"\n        z = x\n        \n        # Apply transformations in reverse order\n        for layer in reversed(self.layers):\n            z = layer.inverse(z)\n        \n        return z\n    \n    def log_prob(self, x):\n        \"\"\"\n        Compute log probability: log p(x)\n        \n        This is the TRAINING OBJECTIVE. We want to maximize this!\n        \n        Uses the change-of-variables formula:\n        log p(x) = log p(z) + log|det J|\n        where z = inverse(x) and J is the Jacobian (derivative matrix)\n        \"\"\"\n        z = self.inverse(x)\n        \n        # Compute forward to get log determinant\n        x_reconstructed, log_det = self.forward(z)\n        \n        # Log probability in base Gaussian\n        log_prob_z = self.base_dist.log_prob(z.squeeze())\n        \n        # Change of variables formula\n        # (Note: we ADD log_det here because we're going from z to x,\n        #  which is the opposite direction from the inverse)\n        return log_prob_z + log_det\n    \n    def sample(self, num_samples):\n        \"\"\"\n        Generate samples from the learned distribution\n        \n        Steps:\n        1. Sample z ~ N(0, 1)  [easy!]\n        2. Apply forward transformations to get x\n        3. x now follows our learned complex distribution!\n        \"\"\"\n        z = self.base_dist.sample((num_samples, 1))\n        x, _ = self.forward(z)\n        return x\n\n\n# =============================================================================\n# EXAMPLE: Transform a Gaussian into a bimodal (two-peaked) distribution\n# =============================================================================\n\ndef make_bimodal_data(n_samples=1000):\n    \"\"\"\n    Create a bimodal distribution (mixture of two Gaussians)\n    \n    This is a common pattern in real data - sometimes parameters or\n    measurements naturally cluster into two groups.\n    \"\"\"\n    # First mode: centered at -2 with std 0.5\n    mode1 = torch.randn(n_samples // 2, 1) * 0.5 - 2\n    \n    # Second mode: centered at +3 with std 0.7\n    mode2 = torch.randn(n_samples // 2, 1) * 0.7 + 3\n    \n    # Combine them\n    data = torch.cat([mode1, mode2], dim=0)\n    \n    return data\n\n\nprint(\"=\"*70)\nprint(\"1D NORMALIZING FLOW - Learning a Bimodal Distribution\")\nprint(\"=\"*70)\nprint(\"\\nGoal: Learn to transform a simple Gaussian (one peak)\")\nprint(\"      into a bimodal distribution (two peaks)\")\nprint(\"\")\n\n# Create training data\nprint(\"Creating bimodal dataset...\")\nprint(\"  - Peak 1: centered at -2.0 with std 0.5\")\nprint(\"  - Peak 2: centered at +3.0 with std 0.7\")\ndata = make_bimodal_data(3000)\n\n# Create the flow\nprint(\"\\nInitializing 1D normalizing flow...\")\nprint(\"  - Number of layers: 6\")\nprint(\"  - Bins per layer: 32\")\nprint(\"  - Each layer learns a piecewise-linear warping function\")\nflow_1d = NormalizingFlow1D(num_layers=6, num_bins=32)\n\n# Optimizer\noptimizer = torch.optim.Adam(flow_1d.parameters(), lr=1e-3)\n\n# Learning rate scheduler - reduce learning rate when loss plateaus\n# This helps the model converge better\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='max', factor=0.5, patience=200, verbose=True\n)\n\n# Training\nprint(\"\\nTraining...\")\nprint(\"  - Loss function: Negative log-likelihood (we want to MAXIMIZE probability)\")\nprint(\"  - Optimization: Adam with learning rate decay\")\nprint(\"\")\n\nnum_epochs = 3000\nbatch_size = 256\n\nfor epoch in range(num_epochs):\n    # Sample a random batch\n    idx = torch.randperm(len(data))[:batch_size]\n    batch = data[idx]\n    \n    # Compute negative log likelihood\n    # We want to MAXIMIZE log p(x), which is equivalent to MINIMIZING -log p(x)\n    log_prob = flow_1d.log_prob(batch)\n    loss = -log_prob.mean()  # Negative log likelihood\n    \n    # Backpropagation and optimization step\n    optimizer.zero_grad()\n    loss.backward()\n    \n    # Clip gradients to prevent instability\n    torch.nn.utils.clip_grad_norm_(flow_1d.parameters(), max_norm=1.0)\n    \n    optimizer.step()\n    \n    # Update learning rate based on performance\n    scheduler.step(log_prob.mean())\n    \n    if (epoch + 1) % 500 == 0:\n        avg_log_prob = log_prob.mean().item()\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        print(f\"  Loss (NLL): {loss.item():.4f}\")\n        print(f\"  Avg log prob: {avg_log_prob:.4f}\")\n        print(f\"  Learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n        print(\"\")\n\nprint(\"Training complete!\")\n\n# =============================================================================\n# VISUALIZE THE RESULTS\n# =============================================================================\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Plot 1: Original training data\nprint(\"\\nGenerating visualizations...\")\naxes[0].hist(data.numpy(), bins=60, density=True, alpha=0.7, color='blue', edgecolor='black')\naxes[0].set_title(\"Original Data (Bimodal)\", fontsize=14, fontweight='bold')\naxes[0].set_xlabel(\"x\", fontsize=12)\naxes[0].set_ylabel(\"Density\", fontsize=12)\naxes[0].set_xlim(-5, 6)\naxes[0].grid(alpha=0.3)\n\n# Plot 2: Samples from the trained flow\nwith torch.no_grad():\n    samples = flow_1d.sample(3000)\naxes[1].hist(samples.numpy(), bins=60, density=True, alpha=0.7, color='orange', edgecolor='black')\naxes[1].set_title(\"Samples from Trained Flow\", fontsize=14, fontweight='bold')\naxes[1].set_xlabel(\"x\", fontsize=12)\naxes[1].set_ylabel(\"Density\", fontsize=12)\naxes[1].set_xlim(-5, 6)\naxes[1].grid(alpha=0.3)\n\n# Plot 3: Show the transformation process\n# Visualize what happens to a Gaussian as we apply each layer\nwith torch.no_grad():\n    z = flow_1d.base_dist.sample((2000, 1))\n    \n    # Show base Gaussian distribution\n    axes[2].hist(z.numpy(), bins=40, density=True, alpha=0.3, \n                label='Base: Gaussian N(0,1)', color='blue', edgecolor='black')\n    \n    # Apply transformations layer by layer\n    x = z\n    for i, layer in enumerate(flow_1d.layers):\n        x, _ = layer(x)\n        # Show intermediate result after half the layers\n        if i == len(flow_1d.layers) // 2 - 1:\n            axes[2].hist(x.numpy(), bins=40, density=True, alpha=0.4,\n                       label=f'After {i+1} layers', color='green', edgecolor='black')\n    \n    # Show final result\n    axes[2].hist(x.numpy(), bins=40, density=True, alpha=0.6,\n               label='Final: Bimodal', color='orange', edgecolor='black')\n\naxes[2].set_title(\"Transformation Process\", fontsize=14, fontweight='bold')\naxes[2].set_xlabel(\"x\", fontsize=12)\naxes[2].set_ylabel(\"Density\", fontsize=12)\naxes[2].legend(fontsize=10)\naxes[2].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"1D FLOW SUMMARY - What Did We Learn?\")\nprint(\"=\"*70)\nprint(\"\\n1. ARCHITECTURE:\")\nprint(\"   - Used 6 piecewise-linear transformation layers\")\nprint(\"   - Each layer has 32 bins (like a flexible histogram)\")\nprint(\"   - Total parameters: ~200 (very lightweight!)\")\nprint(\"\")\nprint(\"2. KEY CONCEPTS:\")\nprint(\"   - Invertibility: Can go forward (z->x) and backward (x->z)\")\nprint(\"   - Monotonicity: Functions always increase (no folding back)\")\nprint(\"   - Change of variables: log p(x) = log p(z) + log|det J|\")\nprint(\"\")\nprint(\"3. WHAT HAPPENED:\")\nprint(\"   - Started with simple Gaussian: one peak at 0\")\nprint(\"   - Applied 6 transformations: warping the distribution\")\nprint(\"   - Result: two peaks at -2 and +3 (bimodal!)\")\nprint(\"\")\nprint(\"4. WHY THIS MATTERS FOR NPE:\")\nprint(\"   - In NPE, we model p(theta|data) - posterior distribution\")\nprint(\"   - Posteriors can be complex (multi-modal, skewed, etc.)\")\nprint(\"   - Normalizing flows can represent these complex shapes\")\nprint(\"   - We can both SAMPLE from them and COMPUTE probabilities\")\nprint(\"\")\nprint(\"5. ADVANTAGES:\")\nprint(\"   + Exact probability computation (unlike GANs)\")\nprint(\"   + Easy sampling (unlike MCMC)\")\nprint(\"   + Flexible (can model complex distributions)\")\nprint(\"   + Efficient (fast forward and inverse passes)\")\nprint(\"=\"*70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}