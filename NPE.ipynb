{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67ba64f2",
   "metadata": {},
   "source": [
    "# Neural Posterior Estimation\n",
    "\n",
    "May the machine spirit bless me in this endeavour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0605072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from JHPY import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aeeef7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number_of_data_points = 1024\n",
    "\n",
    "# outputs = generate_sine_data(data_points=number_of_data_points, mean=10, std=2)\n",
    "\n",
    "# train_loader = outputs[\"Train_Loader\"]\n",
    "# val_loader = outputs[\"Val_Loader\"]\n",
    "# test_loader = outputs[\"Test_Loader\"]\n",
    "\n",
    "# latent_data = np.random.normal(size=number_of_data_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1019b123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just creating some random amplitudes for the time being\n",
    "amplitudes = np.random.normal(10,2,20000)\n",
    "\n",
    "train_data = torch.from_numpy(amplitudes).float().unsqueeze(1)  # Convert to float32 and add dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecce2d5b",
   "metadata": {},
   "source": [
    "Ok so I am going to write a normalising flow BUT without the conditionality of the data for the time being, just a map between latent and amplitude space for the time being"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d229e5",
   "metadata": {},
   "source": [
    "## Coupling Layer\n",
    "\n",
    "A lot of this next bit is from claude, going to get it working then modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "75bdd9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CouplingLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Affine coupling layer for normalizing flows.\n",
    "    \n",
    "    Given input z = [z1, z2]:\n",
    "    - x1 = z1 (unchanged)\n",
    "    - x2 = z2 * exp(s(z1)) + t(z1)  (transformed)\n",
    "    \n",
    "    where s and t are neural networks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim, hidden_dim=128, mask_type='half'):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "        # Create mask (which dimensions to keep unchanged)\n",
    "        mask = torch.zeros(dim)\n",
    "        mask[:dim//2] = 1\n",
    "        self.register_buffer('mask', mask)\n",
    "        \n",
    "        # Scale network (s)\n",
    "        self.scale_net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Tanh()  # Bound output to [-1, 1], then scale\n",
    "        )\n",
    "        \n",
    "        # Translation network (t)\n",
    "        self.translate_net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, dim)\n",
    "        )\n",
    "        \n",
    "        # Scale factor for numerical stability\n",
    "        self.scale_factor = 2.0\n",
    "    \n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Forward: LATENT SPACE (z) → DATA SPACE (x)\n",
    "        \n",
    "        Returns:\n",
    "            x: transformed samples\n",
    "            log_det: log determinant of Jacobian\n",
    "        \"\"\"\n",
    "        # Apply mask to get the unchanged part\n",
    "        z_masked = z * self.mask\n",
    "        \n",
    "        # Compute scale and translation\n",
    "        s = self.scale_net(z_masked) * self.scale_factor  # Scale output\n",
    "        t = self.translate_net(z_masked)\n",
    "        \n",
    "        # Apply transformation: x = z * exp(s) + t for masked dimensions\n",
    "        x = z * self.mask + (1 - self.mask) * (z * torch.exp(s) + t)\n",
    "        \n",
    "        # Log determinant is sum of scales (only for transformed dimensions)\n",
    "        log_det = torch.sum((1 - self.mask) * s, dim=1)\n",
    "        \n",
    "        return x, log_det\n",
    "    \n",
    "    def inverse(self, x):\n",
    "        \"\"\"\n",
    "        Inverse: DATA SPACE (x) → LATENT SPACE (z)\n",
    "        \"\"\"\n",
    "        x_masked = x * self.mask\n",
    "        \n",
    "        s = self.scale_net(x_masked) * self.scale_factor\n",
    "        t = self.translate_net(x_masked)\n",
    "        \n",
    "        # Inverse: z = (x - t) * exp(-s) for masked dimensions\n",
    "        z = x * self.mask + (1 - self.mask) * ((x - t) * torch.exp(-s))\n",
    "        \n",
    "        log_det = -torch.sum((1 - self.mask) * s, dim=1)\n",
    "        \n",
    "        return z, log_det"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6160412",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "32ae2a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizingFlow(nn.Module):\n",
    "    \"\"\"\n",
    "    Normalizing flow: stacks coupling layers to transform distributions.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, n_layers=8, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "        # Base distribution: standard Gaussian in LATENT SPACE\n",
    "        self.register_buffer('base_mean', torch.zeros(dim))\n",
    "        self.register_buffer('base_cov', torch.eye(dim))\n",
    "        \n",
    "        # Stack of coupling layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(n_layers):\n",
    "            layer = CouplingLayer(dim, hidden_dim)\n",
    "            # Alternate mask every other layer\n",
    "            if i % 2 == 1:\n",
    "                layer.mask = 1 - layer.mask\n",
    "            self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        LATENT SPACE → DATA SPACE\n",
    "        Transform z (Gaussian) to x (complex distribution)\n",
    "        \n",
    "        Returns:\n",
    "            x: samples in data space\n",
    "            log_prob: log probability of samples\n",
    "        \"\"\"\n",
    "        log_det_total = torch.zeros(z.shape[0], device=z.device)\n",
    "        x = z\n",
    "        \n",
    "        # Apply each coupling layer\n",
    "        for layer in self.layers:\n",
    "            print(layer)\n",
    "            x, log_det = layer(x)\n",
    "            log_det_total += log_det\n",
    "        \n",
    "        # Compute log probability using change of variables\n",
    "        # log p(x) = log p(z) + log |det(dz/dx)|\n",
    "        base_dist = MultivariateNormal(self.base_mean, self.base_cov)\n",
    "        log_prob_base = base_dist.log_prob(z)\n",
    "        log_prob = log_prob_base + log_det_total\n",
    "        \n",
    "        return x, log_prob\n",
    "    \n",
    "    def inverse(self, x):\n",
    "        \"\"\"\n",
    "        DATA SPACE → LATENT SPACE\n",
    "        Map x back to z\n",
    "        \"\"\"\n",
    "        log_det_total = torch.zeros(x.shape[0], device=x.device)\n",
    "        z = x\n",
    "        \n",
    "        # Apply inverse transformations in reverse order\n",
    "        for layer in reversed(self.layers):\n",
    "            z, log_det = layer.inverse(z)\n",
    "            log_det_total += log_det\n",
    "        \n",
    "        return z, log_det_total\n",
    "    \n",
    "    def log_prob(self, x):\n",
    "        \"\"\"\n",
    "        Compute log probability of x under the flow.\n",
    "        \"\"\"\n",
    "        z, log_det = self.inverse(x)\n",
    "        base_dist = MultivariateNormal(self.base_mean, self.base_cov)\n",
    "        log_prob_base = base_dist.log_prob(z)\n",
    "        return log_prob_base + log_det\n",
    "    \n",
    "    def sample(self, n_samples):\n",
    "        \"\"\"\n",
    "        Generate samples from the learned distribution.\n",
    "        \n",
    "        1. Sample z from LATENT SPACE (standard Gaussian)\n",
    "        2. Transform through flow to DATA SPACE\n",
    "        3. Get samples from complex distribution!\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            base_dist = MultivariateNormal(self.base_mean, self.base_cov)\n",
    "            z = base_dist.sample((n_samples,))\n",
    "            x, _ = self.forward(z)\n",
    "        return x\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eaacb2",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d9374d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAINING THE NORMALIZING FLOW\n",
      "======================================================================\n",
      "\n",
      "Model parameters: 270,352\n",
      "Number of layers: 8\n",
      "Training samples: 20000\n",
      "Batch size: 256\n",
      "Epochs: 100\n",
      "\n",
      "The flow is learning to:\n",
      "  1. Map data space (x) BACK to latent space (z)\n",
      "  2. Ensure that z looks like a standard Gaussian\n",
      "  3. This automatically makes forward mapping (z→x) work!\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 2.1337\n",
      "Epoch 20/100, Loss: 2.1258\n",
      "Epoch 30/100, Loss: 2.1157\n",
      "Epoch 40/100, Loss: 2.1156\n",
      "Epoch 50/100, Loss: 2.1177\n",
      "Epoch 60/100, Loss: 2.1152\n",
      "Epoch 70/100, Loss: 2.1178\n",
      "Epoch 80/100, Loss: 2.1193\n",
      "Epoch 90/100, Loss: 2.1153\n",
      "Epoch 100/100, Loss: 2.1160\n",
      "\n",
      "✓ Training complete!\n",
      "  Initial loss: 2.7114\n",
      "  Final loss:   2.1160\n",
      "  Improvement:  0.5954\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "flow = NormalizingFlow(dim=1, n_layers=8, hidden_dim=128)\n",
    "flow = flow.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(flow.parameters(), lr=1e-3)\n",
    "\n",
    "# Training parameters\n",
    "n_epochs = 100\n",
    "batch_size = 256\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING THE NORMALIZING FLOW\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nModel parameters: {sum(p.numel() for p in flow.parameters()):,}\")\n",
    "print(f\"Number of layers: {len(flow.layers)}\")\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Epochs: {n_epochs}\")\n",
    "\n",
    "print(\"\\nThe flow is learning to:\")\n",
    "print(\"  1. Map data space (x) BACK to latent space (z)\")\n",
    "print(\"  2. Ensure that z looks like a standard Gaussian\")\n",
    "print(\"  3. This automatically makes forward mapping (z→x) work!\\n\")\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_losses = []\n",
    "    \n",
    "    # Shuffle data\n",
    "    perm = torch.randperm(len(train_data))\n",
    "    train_data_shuffled = train_data[perm]\n",
    "    \n",
    "    # Mini-batch training\n",
    "    for i in range(0, len(train_data), batch_size):\n",
    "        batch = train_data_shuffled[i:i+batch_size].to(device)\n",
    "        \n",
    "        # Compute negative log likelihood\n",
    "        log_prob = flow.log_prob(batch)\n",
    "        loss = -log_prob.mean()  # Maximize log prob = minimize negative log prob\n",
    "        \n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(flow.parameters(), 5.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_losses.append(loss.item())\n",
    "    \n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")\n",
    "print(f\"  Initial loss: {losses[0]:.4f}\")\n",
    "print(f\"  Final loss:   {losses[-1]:.4f}\")\n",
    "print(f\"  Improvement:  {losses[0] - losses[-1]:.4f}\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bc5638",
   "metadata": {},
   "source": [
    "# Evaluate the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee17be77",
   "metadata": {},
   "outputs": [],
   "source": "samples = flow.sample(1024)\n\n# Convert histograms to stairs plots with normalization\ncounts1, bins1 = np.histogram(samples.detach().numpy(), bins=30, density=True)\ncounts2, bins2 = np.histogram(amplitudes, bins=30, density=True)\n\nplt.stairs(counts1, bins1, label='Flow samples')\nplt.stairs(counts2, bins2, label='Original amplitudes')\nplt.legend()\nplt.ylabel('Probability Density')\nplt.xlabel('Amplitude')\nplt.show()"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}