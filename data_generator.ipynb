{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7523127",
   "metadata": {},
   "source": [
    "# Using this to develop a function to create large amounts of pycbc data\n",
    "want it to take a dictionary, containing the parameters to vary and their distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0719dba5",
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport torch\nfrom pycbc.waveform import get_td_waveform\nfrom pycbc.detector import Detector\nfrom multiprocessing import Pool, cpu_count\nfrom functools import partial\nfrom typing import Dict, Callable, List, Tuple\nfrom tqdm import tqdm\nfrom torch.utils.data import TensorDataset, DataLoader, random_split"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760c4ed4",
   "metadata": {},
   "outputs": [],
   "source": "def _validate_config(config: Dict[str, Callable]) -> None:\n    \"\"\"Validate configuration dictionary.\"\"\"\n    if not isinstance(config, dict):\n        raise TypeError(\"config must be a dictionary\")\n    \n    if not config:\n        raise ValueError(\"config cannot be empty\")\n    \n    for key, value in config.items():\n        if not callable(value):\n            raise TypeError(f\"config['{key}'] must be callable (e.g., a distribution function)\")\n        \n        try:\n            test = value(size=2)\n            if not isinstance(test, np.ndarray):\n                raise TypeError(f\"config['{key}']() must return numpy array\")\n        except Exception as e:\n            raise ValueError(f\"config['{key}'] failed test call: {e}\")\n\n\ndef _generate_parameter_sets(config: Dict[str, Callable], num_samples: int) -> List[Dict]:\n    \"\"\"Generate all parameter combinations upfront.\"\"\"\n    param_arrays = {}\n    for param_name, dist_func in config.items():\n        param_arrays[param_name] = dist_func(size=num_samples)\n    \n    param_dicts = []\n    for i in range(num_samples):\n        param_dict = {name: float(values[i]) for name, values in param_arrays.items()}\n        param_dicts.append(param_dict)\n    \n    return param_dicts\n\n\ndef _generate_single_waveform(params: Dict, time_resolution: float,\n                              duration: float, approximant: str,\n                              f_lower: float, detectors: List[str]) -> Dict:\n    \"\"\"Worker function to generate a single waveform and project to detectors.\"\"\"\n    try:\n        hp, hc = get_td_waveform(\n            approximant=approximant,\n            mass1=params['mass1'],\n            mass2=params['mass2'],\n            spin1z=params.get('spin1z', 0.0),\n            spin2z=params.get('spin2z', 0.0),\n            inclination=params.get('inclination', 0.0),\n            coa_phase=params.get('coa_phase', 0.0),\n            delta_t=time_resolution,\n            f_lower=f_lower\n        )\n        \n        # Get sky location parameters (defaults to north pole and zero polarization)\n        ra = params.get('ra', 0.0)\n        dec = params.get('dec', np.pi/2)  # North pole\n        polarization = params.get('polarization', 0.0)\n        tc = params.get('tc', 0.0)\n        \n        # Project to detectors\n        detector_signals = {}\n        for det_name in detectors:\n            detector = Detector(det_name)\n            signal = detector.project_wave(hp, hc, ra, dec, polarization, method='lal')\n            detector_signals[det_name] = signal\n        \n        result = {\n            'success': True,\n            'detectors': detector_signals,\n            'params': params\n        }\n        \n        return result\n        \n    except Exception as e:\n        return {'success': False, 'error': str(e), 'params': params}\n\n\ndef _generate_waveforms_parallel(param_dicts: List[Dict],\n                                time_resolution: float,\n                                duration: float,\n                                approximant: str,\n                                f_lower: float,\n                                num_workers: int,\n                                show_progress: bool,\n                                detectors: List[str]) -> List[Dict]:\n    \"\"\"Generate waveforms in parallel using multiprocessing.\"\"\"\n    worker_func = partial(_generate_single_waveform,\n                          time_resolution=time_resolution,\n                          duration=duration,\n                          approximant=approximant,\n                          f_lower=f_lower,\n                          detectors=detectors)\n    \n    with Pool(processes=num_workers) as pool:\n        if show_progress:\n            results = list(tqdm(\n                pool.imap_unordered(worker_func, param_dicts, chunksize=100),\n                total=len(param_dicts),\n                desc=\"Generating waveforms\"\n            ))\n        else:\n            results = list(pool.imap_unordered(worker_func, param_dicts, chunksize=100))\n    \n    return results\n\n\ndef pycbc_data_generator(config: Dict[str, Callable],\n                        num_samples: int,\n                        time_resolution: float = 1/4096,\n                        duration: float = 1.0,\n                        approximant: str = 'IMRPhenomXP',\n                        f_lower: float = 40.0,\n                        num_workers: int = None,\n                        batch_size: int = 256,\n                        chunk_size: int = 10000,\n                        target_length: int = None,\n                        train_split: float = 0.8,\n                        val_split: float = 0.1,\n                        show_progress: bool = True,\n                        detectors: List[str] = None) -> Dict:\n    \"\"\"\n    Generate PyCBC waveforms projected to detectors.\n    Returns PyTorch DataLoaders for training, validation, and testing.\n    \n    Parameters\n    ----------\n    config : dict\n        Dictionary mapping parameter names to numpy distribution functions.\n        \n        Required parameters:\n        - 'mass1': Primary mass (solar masses)\n        - 'mass2': Secondary mass (solar masses)\n        \n        Optional parameters:\n        - 'spin1z', 'spin2z': Spin components\n        - 'inclination', 'coa_phase': Orientation angles\n        - 'ra': Right ascension (radians) - default: 0.0\n        - 'dec': Declination (radians) - default: π/2 (north pole)\n        - 'polarization': Polarization angle (radians) - default: 0.0\n        - 'tc': Coalescence time - default: 0.0\n        \n        Example:\n        config = {\n            'mass1': lambda size: np.random.uniform(10, 50, size=size),\n            'mass2': lambda size: np.random.uniform(10, 50, size=size),\n            'ra': lambda size: np.random.uniform(0, 2*np.pi, size=size),\n            'dec': lambda size: np.random.uniform(-np.pi/2, np.pi/2, size=size),\n            'polarization': lambda size: np.random.uniform(0, 2*np.pi, size=size),\n        }\n        \n    num_samples : int\n        Total number of waveforms to generate\n    time_resolution : float\n        Time step (delta_t). Default: 1/4096\n    approximant : str\n        Waveform approximant. Default: 'IMRPhenomXP'\n    f_lower : float\n        Lower frequency cutoff (Hz). Default: 40.0\n    num_workers : int\n        Parallel processes. Default: min(cpu_count(), 8)\n    batch_size : int\n        DataLoader batch size. Default: 256\n    chunk_size : int\n        Process in chunks for memory. Default: 10000\n    target_length : int\n        Target length for padding. Default: max length\n    train_split : float\n        Training fraction. Default: 0.8\n    val_split : float\n        Validation fraction. Default: 0.1\n    detectors : list of str\n        Detector names. Default: ['H1', 'L1']\n    \n    Returns\n    -------\n    dict with 'train_loader', 'val_loader', 'test_loader', 'metadata'\n        \n    Notes\n    -----\n    If ra, dec, or polarization are not provided in config, they default to:\n    - ra = 0.0\n    - dec = π/2 (north pole)\n    - polarization = 0.0\n    \"\"\"\n    # Validate inputs\n    _validate_config(config)\n    if num_samples <= 0:\n        raise ValueError(\"num_samples must be positive\")\n    if not 0 < train_split < 1 or not 0 < val_split < 1:\n        raise ValueError(\"train_split and val_split must be between 0 and 1\")\n    if train_split + val_split >= 1:\n        raise ValueError(\"train_split + val_split must be < 1\")\n    \n    if num_workers is None:\n        num_workers = min(cpu_count(), 8)\n    \n    # Set default detectors\n    if detectors is None:\n        detectors = ['H1', 'L1']\n    \n    # Check which sky parameters are provided\n    sky_params_provided = {\n        'ra': 'ra' in config,\n        'dec': 'dec' in config,\n        'polarization': 'polarization' in config\n    }\n    \n    if any(sky_params_provided.values()):\n        print(f\"Generating {num_samples} waveforms with projection to {detectors}\")\n        print(f\"  Sky parameters: ra={'provided' if sky_params_provided['ra'] else 'default (0.0)'}, \"\n              f\"dec={'provided' if sky_params_provided['dec'] else 'default (π/2)'}, \"\n              f\"psi={'provided' if sky_params_provided['polarization'] else 'default (0.0)'}\")\n    else:\n        print(f\"Generating {num_samples} waveforms with projection to {detectors}\")\n        print(f\"  Using default sky location: ra=0.0, dec=π/2 (north pole), psi=0.0\")\n    \n    # Generate all parameters upfront\n    param_dicts = _generate_parameter_sets(config, num_samples)\n    \n    # Process in chunks for memory efficiency\n    all_successful = []\n    all_failed = []\n    num_chunks = (num_samples + chunk_size - 1) // chunk_size\n    \n    for chunk_idx in range(num_chunks):\n        chunk_start = chunk_idx * chunk_size\n        chunk_end = min(chunk_start + chunk_size, num_samples)\n        chunk_params = param_dicts[chunk_start:chunk_end]\n        \n        if num_chunks > 1:\n            print(f\"\\nChunk {chunk_idx + 1}/{num_chunks} ({len(chunk_params)} waveforms)...\")\n        \n        # Generate waveforms with detector projection\n        chunk_results = _generate_waveforms_parallel(\n            chunk_params, time_resolution, duration,\n            approximant, f_lower, num_workers, show_progress, detectors\n        )\n        \n        # Single pass: separate and accumulate\n        for r in chunk_results:\n            if r['success']:\n                all_successful.append(r)\n            else:\n                all_failed.append(r)\n        \n        if num_chunks > 1:\n            print(f\"  Chunk: {len([r for r in chunk_results if r['success']])} successful\")\n    \n    if not all_successful:\n        raise RuntimeError(\"No waveforms were successfully generated!\")\n    \n    num_success = len(all_successful)\n    num_failed = len(all_failed)\n    print(f\"\\nGeneration complete: {num_success} successful, {num_failed} failed\")\n    \n    # Extract everything in one pass with pre-allocated arrays\n    print(f\"\\nProcessing {num_success} waveforms...\")\n    \n    # Get parameter names and detector names\n    param_names = list(all_successful[0]['params'].keys())\n    num_params = len(param_names)\n    detector_names = list(all_successful[0]['detectors'].keys())\n    num_detectors = len(detector_names)\n    \n    print(f\"  Detector channels: {detector_names}\")\n    \n    # Pre-scan for lengths\n    lengths = np.array([len(w['detectors'][detector_names[0]]) for w in all_successful])\n    min_len, max_len = lengths.min(), lengths.max()\n    target_length = target_length or max_len\n    \n    print(f\"  Lengths: min={min_len}, max={max_len}, target={target_length}\")\n    \n    # Pre-allocate arrays\n    signal_array = np.zeros((num_success, num_detectors, target_length), dtype=np.float32)\n    param_array = np.zeros((num_success, num_params), dtype=np.float32)\n    \n    # Single pass: extract everything\n    for i, w in enumerate(all_successful):\n        # Extract detector signals\n        for j, det_name in enumerate(detector_names):\n            signal_data = w['detectors'][det_name].numpy()\n            current_len = len(signal_data)\n            \n            # Pad or truncate\n            if current_len <= target_length:\n                start_idx = target_length - current_len\n                signal_array[i, j, start_idx:] = signal_data\n            else:\n                signal_array[i, j] = signal_data[-target_length:]\n        \n        # Extract parameters\n        for j, name in enumerate(param_names):\n            param_array[i, j] = w['params'][name]\n    \n    print(f\"  Converting to PyTorch tensors...\")\n    \n    # Convert to PyTorch\n    X = torch.from_numpy(signal_array)  # (N, num_detectors, T)\n    y = torch.from_numpy(param_array)    # (N, num_params)\n    \n    print(f\"  Tensors: X={X.shape}, y={y.shape}\")\n    \n    # Create dataset and split\n    dataset = TensorDataset(X, y)\n    total_size = len(dataset)\n    train_size = int(train_split * total_size)\n    val_size = int(val_split * total_size)\n    test_size = total_size - train_size - val_size\n    \n    train_data, val_data, test_data = random_split(\n        dataset, [train_size, val_size, test_size]\n    )\n    \n    print(f\"  Splits: train={train_size}, val={val_size}, test={test_size}\")\n    \n    # Create DataLoaders\n    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n    \n    print(f\"\\nReady! DataLoaders with batch_size={batch_size}\")\n    \n    return {\n        'train_loader': train_loader,\n        'val_loader': val_loader,\n        'test_loader': test_loader,\n        'metadata': {\n            'parameter_names': param_names,\n            'num_samples': num_success,\n            'num_failed': num_failed,\n            'waveform_shape': tuple(X.shape[1:]),\n            'channels': detector_names,\n            'train_size': train_size,\n            'val_size': val_size,\n            'test_size': test_size,\n            'batch_size': batch_size,\n            'time_resolution': time_resolution,\n            'approximant': approximant,\n            'f_lower': f_lower,\n            'detectors': detector_names,\n            'target_length': target_length,\n            'original_length_range': (int(min_len), int(max_len)),\n            'chunk_size': chunk_size,\n            'sky_params_provided': sky_params_provided\n        }\n    }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da54d16",
   "metadata": {},
   "outputs": [],
   "source": "# Example 1: Basic usage with default sky location (north pole, zero polarization)\nconfig = {\n    'mass1': lambda size: np.random.uniform(10, 50, size=size),\n    'mass2': lambda size: np.random.uniform(10, 50, size=size),\n    'spin1z': lambda size: np.random.uniform(-0.5, 0.5, size=size),\n}\n\n# Generate with H1 and L1 projection (default detectors)\n# Uses default: ra=0.0, dec=π/2 (north pole), polarization=0.0\nresult = pycbc_data_generator(\n    config, \n    num_samples=100, \n    batch_size=16, \n    num_workers=2\n)\n\n# Access the loaders\ntrain_loader = result['train_loader']\nval_loader = result['val_loader']\ntest_loader = result['test_loader']\n\n# Print metadata\nprint(f\"\\nMetadata:\")\nfor key, value in result['metadata'].items():\n    print(f\"  {key}: {value}\")\n\n# Show first batch\nprint(f\"\\nFirst training batch:\")\nfor waveforms, params in train_loader:\n    print(f\"  Waveforms shape: {waveforms.shape}\")  # (batch, 2, time) - H1 and L1 channels\n    print(f\"  Parameters shape: {params.shape}\")\n    print(f\"  H1 channel: {waveforms[0, 0, :5]}\")\n    print(f\"  L1 channel: {waveforms[0, 1, :5]}\")\n    break"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fvsrrymdoq6",
   "metadata": {},
   "outputs": [],
   "source": "# Example 2: Including sky location parameters (random positions)\nconfig_with_sky = {\n    'mass1': lambda size: np.random.uniform(10, 50, size=size),\n    'mass2': lambda size: np.random.uniform(10, 50, size=size),\n    'spin1z': lambda size: np.random.uniform(-0.5, 0.5, size=size),\n    # Add sky location parameters for varying detector response\n    'ra': lambda size: np.random.uniform(0, 2*np.pi, size=size),\n    'dec': lambda size: np.random.uniform(-np.pi/2, np.pi/2, size=size),\n    'polarization': lambda size: np.random.uniform(0, 2*np.pi, size=size),\n}\n\n# Generate with random sky locations\nresult_sky = pycbc_data_generator(\n    config_with_sky, \n    num_samples=100, \n    batch_size=16,\n    num_workers=2\n)\n\nprint(f\"\\nWith random sky locations:\")\nprint(f\"  Sky params provided: {result_sky['metadata']['sky_params_provided']}\")\nfor waveforms, params in result_sky['train_loader']:\n    print(f\"  Waveforms shape: {waveforms.shape}\")\n    print(f\"  Parameters include: {result_sky['metadata']['parameter_names']}\")\n    break"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g1f456q9ctw",
   "metadata": {},
   "outputs": [],
   "source": "# Example 3: Using three detectors (H1, L1, V1)\nconfig_three = {\n    'mass1': lambda size: np.random.uniform(10, 50, size=size),\n    'mass2': lambda size: np.random.uniform(10, 50, size=size),\n    'spin1z': lambda size: np.random.uniform(-0.5, 0.5, size=size),\n    'ra': lambda size: np.random.uniform(0, 2*np.pi, size=size),\n    'dec': lambda size: np.random.uniform(-np.pi/2, np.pi/2, size=size),\n    'polarization': lambda size: np.random.uniform(0, 2*np.pi, size=size),\n}\n\n# Generate with three detectors\nresult_three = pycbc_data_generator(\n    config_three, \n    num_samples=50, \n    batch_size=16,\n    detectors=['H1', 'L1', 'V1'],  # Add Virgo\n    num_workers=2\n)\n\nprint(f\"\\nWith three detectors:\")\nfor waveforms, params in result_three['train_loader']:\n    print(f\"  Waveforms shape: {waveforms.shape}\")  # (batch, 3, time)\n    print(f\"  Channels: {result_three['metadata']['channels']}\")\n    print(f\"  H1: {waveforms[0, 0, :5]}\")\n    print(f\"  L1: {waveforms[0, 1, :5]}\")\n    print(f\"  V1: {waveforms[0, 2, :5]}\")\n    break"
  },
  {
   "cell_type": "markdown",
   "id": "akb4bcvx2tf",
   "metadata": {},
   "source": "## Example 4: Large-scale production usage\n\nFor generating hundreds of thousands of waveforms:\n\n```python\n# Define comprehensive parameter distributions\nlarge_config = {\n    'mass1': lambda size: np.random.uniform(10, 100, size=size),\n    'mass2': lambda size: np.random.uniform(10, 100, size=size),\n    'spin1z': lambda size: np.random.uniform(-0.9, 0.9, size=size),\n    'spin2z': lambda size: np.random.uniform(-0.9, 0.9, size=size),\n    'inclination': lambda size: np.random.uniform(0, np.pi, size=size),\n    'coa_phase': lambda size: np.random.uniform(0, 2*np.pi, size=size),\n    # Sky location parameters (optional - will use defaults if omitted)\n    'ra': lambda size: np.random.uniform(0, 2*np.pi, size=size),\n    'dec': lambda size: np.random.uniform(-np.pi/2, np.pi/2, size=size),\n    'polarization': lambda size: np.random.uniform(0, 2*np.pi, size=size),\n}\n\n# Generate 100,000 waveforms with H1 and L1 detector projection\n# Takes ~2-3 hours on 8-core system\nresult_large = pycbc_data_generator(\n    large_config,\n    num_samples=100000,\n    batch_size=256,\n    num_workers=8,\n    chunk_size=10000,\n    detectors=['H1', 'L1']\n)\n\n# Directly use in training\ntrain_loader = result_large['train_loader']\nval_loader = result_large['val_loader']\ntest_loader = result_large['test_loader']\n\n# Train with JHPY.py functions\n# from JHPY import ParameterPredictor, train_predictor_model\n# model = ParameterPredictor()\n# train_predictor_model(model, optimizer, loss_fn, n_epochs, train_loader, val_loader)\n```\n\n**Key features:**\n- Always projects to detectors (H1 and L1 by default)\n- If ra/dec/polarization not in config, uses defaults:\n  - ra = 0.0\n  - dec = π/2 (north pole)\n  - polarization = 0.0\n- Output shape: (batch, num_detectors, time)\n- Automatic train/val/test splitting (80/10/10)\n- Memory-efficient chunked processing\n- Ready for JHPY.py training functions"
  },
  {
   "cell_type": "code",
   "id": "de6lhlc9ci",
   "source": "# Test: Does PyCBC respect duration parameter?\nfrom pycbc.waveform import get_td_waveform\n\n# Test with different masses but SAME delta_t\nmasses = [(10, 10), (20, 20), (30, 30), (40, 40), (50, 50)]\n\nprint(\"WITHOUT duration parameter:\")\nfor m1, m2 in masses:\n    hp, hc = get_td_waveform(\n        approximant='IMRPhenomXP',\n        mass1=m1,\n        mass2=m2,\n        delta_t=1/4096,\n        f_lower=40\n    )\n    print(f\"  {m1:2d}+{m2:2d} M☉: {len(hp)} samples\")\n\nprint(\"\\nThe issue: PyCBC generates ONLY the waveform from f_lower to ringdown\")\nprint(\"It doesn't pad to a fixed duration - it's 'just in time' generation\")\nprint(\"\\nSolution: We need to either:\")\nprint(\"  1. Resample all to same length (interpolation)\")\nprint(\"  2. Zero-pad intelligently\") \nprint(\"  3. Use a fixed time window (but this wastes compute/memory)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106e5a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 100000 waveforms using 8 workers...\n",
      "Processing chunk 1/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating waveforms:  71%|███████   | 7101/10000 [04:39<01:55, 25.21it/s]"
     ]
    }
   ],
   "source": [
    "large_config = {\n",
    "    'mass1': lambda size: np.random.uniform(10, 100, size=size),\n",
    "    'mass2': lambda size: np.random.uniform(10, 100, size=size),\n",
    "    'spin1z': lambda size: np.random.uniform(-0.9, 0.9, size=size),\n",
    "    'spin2z': lambda size: np.random.uniform(-0.9, 0.9, size=size),\n",
    "    'inclination': lambda size: np.random.uniform(0, np.pi, size=size),\n",
    "    'coa_phase': lambda size: np.random.uniform(0, 2*np.pi, size=size),\n",
    "}\n",
    "\n",
    "# Generate 100,000 waveforms (will take 2-3 hours on 8-core system)\n",
    "results_large = pycbc_data_generator(\n",
    "    large_config, \n",
    "    datapoints_per_param=100000,\n",
    "    num_workers=8,\n",
    "    chunk_size=10000\n",
    ")\n",
    "\n",
    "# Save to disk\n",
    "arrays_large = extract_waveform_arrays(results_large)\n",
    "np.savez_compressed('waveforms_100k.npz', **arrays_large)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}