"""
Welcome to John-Hamza py. This is a library the functions we are using for our project.
Doc strings are autogenerated as we are very busy.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import itertools
import random
from torch.utils.data import TensorDataset, DataLoader, random_split
from tqdm import tqdm

def generate_sine_data(data_points, mean, std, no_time_steps=360, noise_mean=0.0, noise_std=0.1, batch_size=16):
    """
    Generate synthetic sine wave data with Gaussian noise for training and testing.

    Creates sine wave signals with randomly sampled amplitudes from a normal distribution,
    adds Gaussian noise, and splits the data into train/validation/test sets with DataLoaders.

    Args:
        data_points (int): Number of sine wave signals to generate
        mean (float): Mean of the normal distribution for sampling amplitudes
        std (float): Standard deviation of the normal distribution for sampling amplitudes
        no_time_steps (int, optional): Number of time steps in each signal. Defaults to 360.
        noise_mean (float, optional): Mean of Gaussian noise (currently overridden in code). Defaults to 0.0.
        noise_std (float, optional): Standard deviation of Gaussian noise (currently overridden in code). Defaults to 0.1.
        batch_size (int, optional): Batch size for DataLoaders. Defaults to 16.

    Returns:
        dict: Dictionary containing:
            - 'Amplitudes' (np.ndarray): Array of sampled amplitudes
            - 'Signals' (np.ndarray): Clean sine wave signals
            - 'Noised_Signals' (np.ndarray): Signals with added Gaussian noise
            - 'Train_Loader' (DataLoader): Training data loader (80% of data)
            - 'Test_Loader' (DataLoader): Test data loader (10% of data)
            - 'Val_Loader' (DataLoader): Validation data loader (10% of data)

    Note:
        The time steps span from 0 to 120 linearly.
        The noise parameters are currently hardcoded to mean=0 and std=0.5 in the function body,
        ignoring the noise_mean and noise_std parameters.
    """

    amplitudes = np.random.normal(mean, std, data_points)

    time_steps = np.linspace(0,120,no_time_steps)

    signals = np.array([np.sin(time_steps)*a for a in amplitudes])

    # adding noise, just gaussian for the time being
    noise_mean = 0
    noise_std = 0.5
    noised_signals = np.array([x + np.random.normal(noise_mean, noise_std, no_time_steps) for x in signals])

    # convert to pytorch datasets
    X = torch.FloatTensor(noised_signals)
    y = torch.FloatTensor(amplitudes)

    data = TensorDataset(X,y)

    train_data, val_data, test_data = random_split(data, lengths=[0.8,0.1,0.1])

    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)
    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)
    
    return {"Amplitudes": amplitudes, "Signals": signals, "Noised_Signals": noised_signals,
            "Train_Loader": train_loader, "Test_Loader": test_loader, "Val_Loader": val_loader}
    
    
def train_model(model, optimizer, loss_fcn, n_epochs, train_dloader, val_dloader, start_epoch = 0, patience = 8, scheduler=None, save_best_model=False, model_path='best_model.pt'):
    """
    Train a PyTorch model with validation monitoring, early stopping, and optional model checkpointing.

    Trains the model for a specified number of epochs, tracking training and validation losses
    and metrics (MAE, RMSE, R²). Implements early stopping based on validation loss and
    optionally saves the best model checkpoint.

    Args:
        model (nn.Module): PyTorch model to train
        optimizer (torch.optim.Optimizer): Optimizer for training
        loss_fcn (callable): Loss function (e.g., nn.MSELoss())
        n_epochs (int): Number of epochs to train
        train_dloader (DataLoader): DataLoader for training data
        val_dloader (DataLoader): DataLoader for validation data
        start_epoch (int, optional): Starting epoch number (for resuming training). Defaults to 0.
        patience (int, optional): Number of epochs to wait for improvement before early stopping. Defaults to 8.
        scheduler (torch.optim.lr_scheduler, optional): Learning rate scheduler. Defaults to None.
        save_best_model (bool, optional): Whether to save checkpoints of the best model. Defaults to False.
        model_path (str, optional): Path to save the best model checkpoint. Defaults to 'best_model.pt'.

    Returns:
        dict: Dictionary containing:
            - 'train_losses' (list): Training losses for each epoch
            - 'val_losses' (list): Validation losses for each epoch
            - 'train_metrics' (list): Training metrics (MAE, RMSE, R²) for each epoch
            - 'val_metrics' (list): Validation metrics (MAE, RMSE, R²) for each epoch
            - 'best_val_loss' (float): Best validation loss achieved
            - 'best_val_epoch' (int): Epoch number where best validation loss occurred

    Note:
        The function prints progress information for each epoch including losses and metrics.
        If a scheduler is provided, it steps based on validation loss.
        Training stops early if validation loss doesn't improve for 'patience' epochs.
    """
    train_losses, val_losses = [], []
    train_metrics, val_metrics = [], []
    best_val_loss = float('inf')
    best_val_epoch = 0

    for epoch in range(start_epoch, start_epoch + n_epochs):
        model.train()
        tloss, vloss = 0, 0
        train_predictions = []
        train_targets = []

        for X_train, y_train in tqdm(train_dloader, desc='Epoch {}, training'.format(epoch+1)):
            optimizer.zero_grad()
            pred = model(X_train)
            loss = loss_fcn(pred, y_train.flatten())            
            tloss += loss.item()
            loss.backward()
            optimizer.step()
            
            train_predictions.extend(pred.detach().numpy())
            train_targets.extend(y_train.flatten().numpy())

            
        model.eval()
        vloss = 0
        val_predictions = []
        val_targets = []

        with torch.no_grad():
            vtrue_preds, vcount = 0., 0
            for X_valid, y_valid in tqdm(val_dloader, desc='Epoch {}, validation'.format(epoch+1)):
                pred = model(X_valid)
                loss = loss_fcn(pred, y_valid.flatten())
                vloss += loss.item()


                val_predictions.extend(pred.numpy())
                val_targets.extend(y_valid.flatten().numpy())

        # Calculate metrics
        train_metrics_dict = calculate_metrics(
            np.array(train_predictions), 
            np.array(train_targets)
        )
        val_metrics_dict = calculate_metrics(
            np.array(val_predictions), 
            np.array(val_targets)
        )

        # Store losses
        avg_train_loss = tloss / len(train_dloader)
        avg_val_loss = vloss / len(val_dloader)
        train_losses.append(avg_train_loss)
        val_losses.append(avg_val_loss)
        
        # Store metrics
        train_metrics.append(train_metrics_dict)
        val_metrics.append(val_metrics_dict)

        # Print epoch results
        print(f"\n[Epoch {epoch+1:2d}]")
        print(f"Training - Loss: {avg_train_loss:.4f}, MAE: {train_metrics_dict['mae']:.4f}, "
              f"RMSE: {train_metrics_dict['rmse']:.4f}, R²: {train_metrics_dict['r2']:.4f}")
        print(f"Validation - Loss: {avg_val_loss:.4f}, MAE: {val_metrics_dict['mae']:.4f}, "
              f"RMSE: {val_metrics_dict['rmse']:.4f}, R²: {val_metrics_dict['r2']:.4f} \n")

        # Learning rate scheduling
        if scheduler is not None:
            scheduler.step(avg_val_loss)
            current_lr = [param_group['lr'] for param_group in optimizer.param_groups]
            print(f"Current learning rates: {current_lr}")

        # Early stopping check
        if avg_val_loss < best_val_loss:
            print("New best validation performance \n")
            best_val_loss = avg_val_loss
            best_val_epoch = epoch
            
            # Save the best model
            if save_best_model:
                checkpoint = {
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'best_val_loss': best_val_loss,
                    'model_config': model.config if hasattr(model, 'config') else None,
                    'train_losses': train_losses,
                    'val_losses': val_losses,
                    'train_metrics': train_metrics,
                    'val_metrics': val_metrics
                }
                torch.save(checkpoint, model_path)
                print(f"Model checkpoint saved to {model_path}\n")
                
        elif best_val_epoch <= epoch - patience:
            print(f'No improvement in validation loss in last {patience} epochs \n')
            break

    return {
        'train_losses': train_losses,
        'val_losses': val_losses,
        'train_metrics': train_metrics,
        'val_metrics': val_metrics,
        'best_val_loss': best_val_loss,
        'best_val_epoch': best_val_epoch
    }

def calculate_metrics(predictions, targets):
    """
    Calculate regression metrics for parameter estimation.

    Computes Mean Absolute Error (MAE), Root Mean Squared Error (RMSE),
    and R² (coefficient of determination) for evaluating regression model performance.

    Args:
        predictions (np.ndarray): Model predictions
        targets (np.ndarray): True target values

    Returns:
        dict: Dictionary containing:
            - 'mae' (float): Mean Absolute Error
            - 'rmse' (float): Root Mean Squared Error
            - 'r2' (float): R² score (coefficient of determination)

    Note:
        R² ranges from -∞ to 1, where 1 indicates perfect prediction,
        0 indicates the model performs as well as a simple mean,
        and negative values indicate worse performance than the mean.
    """
    mae = np.mean(np.abs(predictions - targets))
    rmse = np.sqrt(np.mean((predictions - targets) ** 2))
    
    # R² score
    ss_res = np.sum((targets - predictions) ** 2)
    ss_tot = np.sum((targets - np.mean(targets)) ** 2)
    r2 = 1 - (ss_res / ss_tot)
    
    return {
        'mae': mae,
        'rmse': rmse,
        'r2': r2
    }

def load_best_model(model_path='best_model.pt'):
    """
    Load the best saved model checkpoint from disk.

    Loads a ParameterPredictor model from a saved checkpoint file, including
    the model state, configuration, and training history.

    Args:
        model_path (str, optional): Path to the saved model checkpoint. Defaults to 'best_model.pt'.

    Returns:
        tuple: A tuple containing:
            - model (ParameterPredictor): Loaded model in evaluation mode
            - checkpoint (dict): Full checkpoint dictionary containing:
                - 'epoch' (int): Epoch number when model was saved
                - 'model_state_dict' (dict): Model state dictionary
                - 'optimizer_state_dict' (dict): Optimizer state dictionary
                - 'best_val_loss' (float): Best validation loss achieved
                - 'model_config' (dict): Model configuration
                - 'train_losses' (list): Training loss history
                - 'val_losses' (list): Validation loss history
                - 'train_metrics' (list): Training metrics history
                - 'val_metrics' (list): Validation metrics history

    Note:
        Prints information about the loaded model including epoch number
        and best validation loss achieved.
    """
    checkpoint = torch.load(model_path, weights_only=False)
    
    # Recreate model with saved config
    model = ParameterPredictor(checkpoint['model_config'])
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    print(f"Loaded model from {model_path}")
    print(f"  Best epoch: {checkpoint['epoch'] + 1}")
    print(f"  Best validation loss: {checkpoint['best_val_loss']:.4f}")
    
    return model, checkpoint

def hyperparameter_search(param_grid, train_loader, val_loader, n_epochs=20, n_trials=None):
    """
    Search for the best hyperparameter configuration.
    
    Args:
        param_grid: Dictionary of parameter names to lists of values to try
        train_loader: Training data loader
        val_loader: Validation data loader
        n_epochs: Number of epochs to train each configuration
        n_trials: Number of random configurations to try (None = try all combinations)
    
    Returns:
        best_config: Dictionary with the best configuration
        results: List of dictionaries with all trial results
    """

    
    results = []
    best_val_loss = float('inf')
    best_config = None
    
    # Generate all combinations or sample randomly
    param_names = list(param_grid.keys())
    param_values = [param_grid[name] for name in param_names]
    
    if n_trials is None:
        # Try all combinations (grid search)
        all_combinations = list(itertools.product(*param_values))
    else:
        # Random search: sample n_trials random combinations
        all_combinations = []
        for _ in range(n_trials):
            combo = tuple(random.choice(values) for values in param_values)
            all_combinations.append(combo)
    
    print(f"Testing {len(all_combinations)} configurations...\n")
    
    for i, combo in enumerate(all_combinations):
        # Create config dictionary
        config = dict(zip(param_names, combo))
        
        print(f"{'='*60}")
        print(f"Trial {i+1}/{len(all_combinations)}")
        print(f"Config: {config}")
        print(f"{'='*60}")
        
        # Create model with this configuration
        model = ParameterPredictor(config)
        
        # Create optimizer and scheduler
        lr = config.get('learning_rate', 0.01)
        optimizer = torch.optim.Adam(model.parameters(), lr=lr)
        
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='min',
            factor=0.5,
            patience=2,
            min_lr=1e-6,
        )
        
        loss_fcn = nn.MSELoss()
        
        # Train the model
        try:
            outputs = train_model(
                model, 
                optimizer, 
                loss_fcn, 
                n_epochs, 
                train_loader, 
                val_loader, 
                patience=8,
                scheduler=scheduler
            )
            
            # Get best validation loss
            final_val_loss = min(outputs['val_losses'])
            final_val_metrics = outputs['val_metrics'][outputs['val_losses'].index(final_val_loss)]
            
            # Store results
            result = {
                'config': config.copy(),
                'best_val_loss': final_val_loss,
                'best_val_mae': final_val_metrics['mae'],
                'best_val_rmse': final_val_metrics['rmse'],
                'best_val_r2': final_val_metrics['r2'],
                'n_epochs_trained': len(outputs['val_losses'])
            }
            results.append(result)
            
            print(f"\nFinal validation loss: {final_val_loss:.4f}")
            print(f"Best validation R²: {final_val_metrics['r2']:.4f}\n")
            
            # Update best configuration
            if final_val_loss < best_val_loss:
                best_val_loss = final_val_loss
                best_config = config.copy()
                print(f"*** New best configuration found! ***\n")
        
        except Exception as e:
            print(f"Error training with config {config}: {e}\n")
            continue
    
    # Print summary
    print(f"\n{'='*60}")
    print("HYPERPARAMETER SEARCH COMPLETE")
    print(f"{'='*60}")
    print(f"\nBest configuration:")
    for key, value in best_config.items():
        print(f"  {key}: {value}")
    print(f"\nBest validation loss: {best_val_loss:.4f}")
    
    # Sort results by validation loss
    results.sort(key=lambda x: x['best_val_loss'])
    
    return best_config, results


class ParameterPredictor(nn.Module):
    """
    LSTM-based neural network for parameter prediction from time series data.

    A configurable PyTorch model that uses an LSTM to process sequential data
    followed by fully connected layers for regression tasks. Designed for
    predicting scalar parameters from time series signals.

    Attributes:
        config (dict): Configuration dictionary containing model hyperparameters
        lstm (nn.LSTM): LSTM layer(s) for processing sequential data
        fc (nn.Sequential): Fully connected layers for final prediction

    Default Configuration:
        - lstm_hidden_size (int): 256 - Hidden size of LSTM layers
        - lstm_num_layers (int): 1 - Number of stacked LSTM layers
        - fc_layer_sizes (list): [128, 64] - Sizes of fully connected layers
        - activation (str): 'silu' - Activation function ('silu', 'relu', or 'tanh')
        - dropout (float): 0.0 - Dropout probability for regularization

    Example:
        >>> config = {'lstm_hidden_size': 128, 'dropout': 0.2}
        >>> model = ParameterPredictor(config)
        >>> output = model(input_tensor)  # input shape: [batch, sequence_length]
    """
    def __init__(self, config=None):
        """
        Initialize the ParameterPredictor model.

        Args:
            config (dict, optional): Configuration dictionary to override default hyperparameters.
                Can include:
                - 'lstm_hidden_size' (int): Hidden size of LSTM
                - 'lstm_num_layers' (int): Number of LSTM layers
                - 'fc_layer_sizes' (list): Sizes of FC layers
                - 'activation' (str): Activation function type
                - 'dropout' (float): Dropout probability
                Defaults to None, which uses all default values.
        """
        super().__init__()
        
        # Default configuration
        default_config = {
            'lstm_hidden_size': 256,
            'lstm_num_layers': 1,
            'fc_layer_sizes': [128, 64],  # Sizes of fully connected layers before output
            'activation': 'silu',  # 'silu', 'relu', 'tanh'
            'dropout': 0.0,  # Dropout probability
        }
        
        # Merge with provided config
        if config is None:
            config = {}
        self.config = {**default_config, **config}
        
        # Build LSTM
        self.lstm = nn.LSTM(
            input_size=1, 
            hidden_size=self.config['lstm_hidden_size'], 
            num_layers=self.config['lstm_num_layers'], 
            batch_first=True,
            dropout=self.config['dropout'] if self.config['lstm_num_layers'] > 1 else 0.0
        )
        
        # Build fully connected layers
        fc_layers = []
        input_size = self.config['lstm_hidden_size']
        
        for hidden_size in self.config['fc_layer_sizes']:
            fc_layers.append(nn.Linear(input_size, hidden_size))
            
            # Add activation
            if self.config['activation'] == 'silu':
                fc_layers.append(nn.SiLU())
            elif self.config['activation'] == 'relu':
                fc_layers.append(nn.ReLU())
            elif self.config['activation'] == 'tanh':
                fc_layers.append(nn.Tanh())
            
            # Add dropout if specified
            if self.config['dropout'] > 0:
                fc_layers.append(nn.Dropout(self.config['dropout']))
            
            input_size = hidden_size
        
        # Output layer
        fc_layers.append(nn.Linear(input_size, 1))
        
        self.fc = nn.Sequential(*fc_layers)
    
    def forward(self, x):
        """
        Forward pass through the model.

        Processes input time series through LSTM layers and fully connected layers
        to produce a scalar prediction for each sample in the batch.

        Args:
            x (torch.Tensor): Input tensor of shape [batch, sequence_length]
                containing time series data

        Returns:
            torch.Tensor: Output predictions of shape [batch] containing
                scalar predictions for each sample

        Note:
            The input is automatically reshaped to [batch, sequence, 1] for LSTM processing.
            The final LSTM hidden state is used for prediction.
        """
        # Reshape input to [batch, sequence, features]
        x = x.unsqueeze(-1)  # Add feature dimension: [batch, 4000, 1]
        lstm_out, _ = self.lstm(x)
        # Use last output for prediction
        last_out = lstm_out[:, -1, :]
        return self.fc(last_out).squeeze(-1)