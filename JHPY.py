"""
Welcome to John-Hamza py. This is a library the functions we are using for our project.
Doc strings are autogenerated as we are very busy.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import itertools
import random
from torch.utils.data import TensorDataset, DataLoader, random_split
from tqdm import tqdm

################### Miscellaneous functions ###################

def generate_sine_data(data_points, mean_amplitude, amplitude_std, no_time_steps=360, noise_mean=0.0, noise_std=0.1, batch_size=16):
    """
    Generate synthetic sine wave data with noise and return train/val/test DataLoaders.

    Args:
        data_points (int): Number of sine waves to generate
        mean (float): Mean of amplitude distribution
        std (float): Std dev of amplitude distribution
        no_time_steps (int, optional): Time steps per signal. Defaults to 360.
        noise_mean (float, optional): Ignored; hardcoded to 0. Defaults to 0.0.
        noise_std (float, optional): Ignored; hardcoded to 0.5. Defaults to 0.1.
        batch_size (int, optional): Batch size for loaders. Defaults to 16.

    Returns:
        dict: Contains 'Amplitudes', 'Signals', 'Noised_Signals', 'Train_Loader', 'Test_Loader', 'Val_Loader'.
    """

    amplitudes = np.random.normal(mean_amplitude, amplitude_std, data_points)

    time_steps = np.linspace(0,120,no_time_steps)

    signals = np.array([np.sin(time_steps)*a for a in amplitudes])

    # adding noise, just gaussian for the time being
    noise_mean = 0
    noise_std = 0.5
    noised_signals = np.array([x + np.random.normal(noise_mean, noise_std, no_time_steps) for x in signals])

    # convert to pytorch datasets
    X = torch.FloatTensor(noised_signals)
    y = torch.FloatTensor(amplitudes)

    data = TensorDataset(X,y)

    train_data, val_data, test_data = random_split(data, lengths=[0.8,0.1,0.1])

    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)
    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)
    
    return {"Amplitudes": amplitudes, "Signals": signals, "Noised_Signals": noised_signals,
            "Train_Loader": train_loader, "Test_Loader": test_loader, "Val_Loader": val_loader}
    

################### Neural Network Functions ###################
    
def train_model(model, optimizer, loss_fcn, n_epochs, train_dloader, val_dloader, start_epoch = 0, patience = 8, scheduler=None, save_best_model=False, model_path='best_model.pt'):
    """
    Train model with early stopping, validation monitoring, and optional checkpointing.

    Args:
        model (nn.Module): PyTorch model to train
        optimizer (torch.optim.Optimizer): Optimizer for training
        loss_fcn (callable): Loss function
        n_epochs (int): Number of epochs to train
        train_dloader (DataLoader): Training data loader
        val_dloader (DataLoader): Validation data loader
        start_epoch (int, optional): Starting epoch for resume. Defaults to 0.
        patience (int, optional): Epochs to wait before early stopping. Defaults to 8.
        scheduler (torch.optim.lr_scheduler, optional): LR scheduler. Defaults to None.
        save_best_model (bool, optional): Save best model checkpoint. Defaults to False.
        model_path (str, optional): Path for checkpoint. Defaults to 'best_model.pt'.

    Returns:
        dict: Contains 'train_losses', 'val_losses', 'train_metrics', 'val_metrics', 'best_val_loss', 'best_val_epoch'.
    """
    train_losses, val_losses = [], []
    train_metrics, val_metrics = [], []
    best_val_loss = float('inf')
    best_val_epoch = 0

    for epoch in range(start_epoch, start_epoch + n_epochs):
        model.train()
        tloss, vloss = 0, 0
        train_predictions = []
        train_targets = []

        for X_train, y_train in tqdm(train_dloader, desc='Epoch {}, training'.format(epoch+1)):
            optimizer.zero_grad()
            pred = model(X_train)
            loss = loss_fcn(pred, y_train.flatten())            
            tloss += loss.item()
            loss.backward()
            optimizer.step()
            
            train_predictions.extend(pred.detach().numpy())
            train_targets.extend(y_train.flatten().numpy())

            
        model.eval()
        vloss = 0
        val_predictions = []
        val_targets = []

        with torch.no_grad():
            vtrue_preds, vcount = 0., 0
            for X_valid, y_valid in tqdm(val_dloader, desc='Epoch {}, validation'.format(epoch+1)):
                pred = model(X_valid)
                loss = loss_fcn(pred, y_valid.flatten())
                vloss += loss.item()


                val_predictions.extend(pred.numpy())
                val_targets.extend(y_valid.flatten().numpy())

        # Calculate metrics
        train_metrics_dict = calculate_metrics(
            np.array(train_predictions), 
            np.array(train_targets)
        )
        val_metrics_dict = calculate_metrics(
            np.array(val_predictions), 
            np.array(val_targets)
        )

        # Store losses
        avg_train_loss = tloss / len(train_dloader)
        avg_val_loss = vloss / len(val_dloader)
        train_losses.append(avg_train_loss)
        val_losses.append(avg_val_loss)
        
        # Store metrics
        train_metrics.append(train_metrics_dict)
        val_metrics.append(val_metrics_dict)

        # Print epoch results
        print(f"\n[Epoch {epoch+1:2d}]")
        print(f"Training - Loss: {avg_train_loss:.4f}, MAE: {train_metrics_dict['mae']:.4f}, "
              f"RMSE: {train_metrics_dict['rmse']:.4f}, R²: {train_metrics_dict['r2']:.4f}")
        print(f"Validation - Loss: {avg_val_loss:.4f}, MAE: {val_metrics_dict['mae']:.4f}, "
              f"RMSE: {val_metrics_dict['rmse']:.4f}, R²: {val_metrics_dict['r2']:.4f} \n")

        # Learning rate scheduling
        if scheduler is not None:
            scheduler.step(avg_val_loss)
            current_lr = [param_group['lr'] for param_group in optimizer.param_groups]
            print(f"Current learning rates: {current_lr}")

        # Early stopping check
        if avg_val_loss < best_val_loss:
            print("New best validation performance \n")
            best_val_loss = avg_val_loss
            best_val_epoch = epoch
            
            # Save the best model
            if save_best_model:
                checkpoint = {
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'best_val_loss': best_val_loss,
                    'model_config': model.config if hasattr(model, 'config') else None,
                    'train_losses': train_losses,
                    'val_losses': val_losses,
                    'train_metrics': train_metrics,
                    'val_metrics': val_metrics
                }
                torch.save(checkpoint, model_path)
                print(f"Model checkpoint saved to {model_path}\n")
                
        elif best_val_epoch <= epoch - patience:
            print(f'No improvement in validation loss in last {patience} epochs \n')
            break

    return {
        'train_losses': train_losses,
        'val_losses': val_losses,
        'train_metrics': train_metrics,
        'val_metrics': val_metrics,
        'best_val_loss': best_val_loss,
        'best_val_epoch': best_val_epoch
    }

def calculate_metrics(predictions, targets):
    """
    Calculate MAE, RMSE, and R² metrics.

    Args:
        predictions (np.ndarray): Model predictions
        targets (np.ndarray): Target values

    Returns:
        dict: Contains 'mae', 'rmse', 'r2'.
    """
    mae = np.mean(np.abs(predictions - targets))
    rmse = np.sqrt(np.mean((predictions - targets) ** 2))
    
    # R² score
    ss_res = np.sum((targets - predictions) ** 2)
    ss_tot = np.sum((targets - np.mean(targets)) ** 2)
    r2 = 1 - (ss_res / ss_tot)
    
    return {
        'mae': mae,
        'rmse': rmse,
        'r2': r2
    }

def load_best_model(model_path='best_model.pt'):
    """
    Load saved model checkpoint.

    Args:
        model_path (str, optional): Path to checkpoint. Defaults to 'best_model.pt'.

    Returns:
        tuple: (model, checkpoint dict)
    """
    checkpoint = torch.load(model_path, weights_only=False)
    
    # Recreate model with saved config
    model = ParameterPredictor(checkpoint['model_config'])
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    print(f"Loaded model from {model_path}")
    print(f"  Best epoch: {checkpoint['epoch'] + 1}")
    print(f"  Best validation loss: {checkpoint['best_val_loss']:.4f}")
    
    return model, checkpoint

def hyperparameter_search(param_grid, train_loader, val_loader, n_epochs=20, n_trials=None):
    """
    Search for best hyperparameter configuration.

    Args:
        param_grid: Dict of parameter names to value lists
        train_loader: Training data loader
        val_loader: Validation data loader
        n_epochs: Epochs per configuration. Defaults to 20.
        n_trials: Random trials to try; None = all combinations. Defaults to None.

    Returns:
        tuple: (best_config dict, results list)
    """

    
    results = []
    best_val_loss = float('inf')
    best_config = None
    
    # Generate all combinations or sample randomly
    param_names = list(param_grid.keys())
    param_values = [param_grid[name] for name in param_names]
    
    if n_trials is None:
        # Try all combinations (grid search)
        all_combinations = list(itertools.product(*param_values))
    else:
        # Random search: sample n_trials random combinations
        all_combinations = []
        for _ in range(n_trials):
            combo = tuple(random.choice(values) for values in param_values)
            all_combinations.append(combo)
    
    print(f"Testing {len(all_combinations)} configurations...\n")
    
    for i, combo in enumerate(all_combinations):
        # Create config dictionary
        config = dict(zip(param_names, combo))
        
        print(f"{'='*60}")
        print(f"Trial {i+1}/{len(all_combinations)}")
        print(f"Config: {config}")
        print(f"{'='*60}")
        
        # Create model with this configuration
        model = ParameterPredictor(config)
        
        # Create optimizer and scheduler
        lr = config.get('learning_rate', 0.01)
        optimizer = torch.optim.Adam(model.parameters(), lr=lr)
        
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='min',
            factor=0.5,
            patience=2,
            min_lr=1e-6,
        )
        
        loss_fcn = nn.MSELoss()
        
        # Train the model
        try:
            outputs = train_model(
                model, 
                optimizer, 
                loss_fcn, 
                n_epochs, 
                train_loader, 
                val_loader, 
                patience=8,
                scheduler=scheduler
            )
            
            # Get best validation loss
            final_val_loss = min(outputs['val_losses'])
            final_val_metrics = outputs['val_metrics'][outputs['val_losses'].index(final_val_loss)]
            
            # Store results
            result = {
                'config': config.copy(),
                'best_val_loss': final_val_loss,
                'best_val_mae': final_val_metrics['mae'],
                'best_val_rmse': final_val_metrics['rmse'],
                'best_val_r2': final_val_metrics['r2'],
                'n_epochs_trained': len(outputs['val_losses'])
            }
            results.append(result)
            
            print(f"\nFinal validation loss: {final_val_loss:.4f}")
            print(f"Best validation R²: {final_val_metrics['r2']:.4f}\n")
            
            # Update best configuration
            if final_val_loss < best_val_loss:
                best_val_loss = final_val_loss
                best_config = config.copy()
                print(f"*** New best configuration found! ***\n")
        
        except Exception as e:
            print(f"Error training with config {config}: {e}\n")
            continue
    
    # Print summary
    print(f"\n{'='*60}")
    print("HYPERPARAMETER SEARCH COMPLETE")
    print(f"{'='*60}")
    print(f"\nBest configuration:")
    for key, value in best_config.items():
        print(f"  {key}: {value}")
    print(f"\nBest validation loss: {best_val_loss:.4f}")
    
    # Sort results by validation loss
    results.sort(key=lambda x: x['best_val_loss'])
    
    return best_config, results
################### Neural Network Layers ###################

################### Models ###################

class ParameterPredictor(nn.Module):
    """
    LSTM-based neural network for predicting scalar parameters from time series.

    Configurable model with LSTM layers followed by fully connected layers.
    Config options: lstm_hidden_size (256), lstm_num_layers (1), fc_layer_sizes ([128, 64]),
    activation ('silu'/'relu'/'tanh'), dropout (0.0).
    """
    def __init__(self, config=None):
        """
        Initialize model with optional config overrides.

        Args:
            config (dict, optional): Config dict with lstm_hidden_size, lstm_num_layers, fc_layer_sizes, activation, dropout.
        """
        super().__init__()
        
        # Default configuration
        default_config = {
            'lstm_hidden_size': 256,
            'lstm_num_layers': 1,
            'fc_layer_sizes': [128, 64],  # Sizes of fully connected layers before output
            'activation': 'silu',  # 'silu', 'relu', 'tanh'
            'dropout': 0.0,  # Dropout probability
        }
        
        # Merge with provided config
        if config is None:
            config = {}
        self.config = {**default_config, **config}
        
        # Build LSTM
        self.lstm = nn.LSTM(
            input_size=1, 
            hidden_size=self.config['lstm_hidden_size'], 
            num_layers=self.config['lstm_num_layers'], 
            batch_first=True,
            dropout=self.config['dropout'] if self.config['lstm_num_layers'] > 1 else 0.0
        )
        
        # Build fully connected layers
        fc_layers = []
        input_size = self.config['lstm_hidden_size']
        
        for hidden_size in self.config['fc_layer_sizes']:
            fc_layers.append(nn.Linear(input_size, hidden_size))
            
            # Add activation
            if self.config['activation'] == 'silu':
                fc_layers.append(nn.SiLU())
            elif self.config['activation'] == 'relu':
                fc_layers.append(nn.ReLU())
            elif self.config['activation'] == 'tanh':
                fc_layers.append(nn.Tanh())
            
            # Add dropout if specified
            if self.config['dropout'] > 0:
                fc_layers.append(nn.Dropout(self.config['dropout']))
            
            input_size = hidden_size
        
        # Output layer
        fc_layers.append(nn.Linear(input_size, 1))
        
        self.fc = nn.Sequential(*fc_layers)
    
    def forward(self, x):
        """
        Forward pass: process time series through LSTM and FC layers.

        Args:
            x (torch.Tensor): Input shape [batch, sequence_length]

        Returns:
            torch.Tensor: Output shape [batch] with scalar predictions
        """
        # Reshape input to [batch, sequence, features]
        x = x.unsqueeze(-1)  # Add feature dimension: [batch, 4000, 1]
        lstm_out, _ = self.lstm(x)
        # Use last output for prediction
        last_out = lstm_out[:, -1, :]
        return self.fc(last_out).squeeze(-1)