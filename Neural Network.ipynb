{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99f8ab30",
   "metadata": {},
   "source": [
    "# Feature Extraction from Time Series Data Using a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cebee8",
   "metadata": {},
   "source": [
    "Currently not using pycbc data yet as constructing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a2e483",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Generate simple data -> Done\n",
    "- Add noise -> Done\n",
    "- Build NN model for parameter estimation\n",
    "- Training loop -> Done I think\n",
    "- Testing of trained model\n",
    "- Update to pycbc noise\n",
    "- Update Model to do parameter prosterior prediction, look at dingo\n",
    "- Update to pycbc data\n",
    "- Finished?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3e6b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38ab5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just generating some datasets for the time being\n",
    "data_samples = 1000\n",
    "mean = 10\n",
    "std = 2\n",
    "no_time_steps = 4000\n",
    "\n",
    "amplitudes = np.random.normal(mean, std, data_samples)\n",
    "\n",
    "time_steps = np.linspace(0,120,no_time_steps)\n",
    "\n",
    "signals = np.array([np.sin(time_steps)*a for a in amplitudes])\n",
    "\n",
    "# adding noise, just gaussian for the time being\n",
    "noise_mean = 1\n",
    "noise_std = 2\n",
    "noised_signals = np.array([x + np.random.normal(noise_mean, noise_std, no_time_steps) for x in signals])\n",
    "\n",
    "# convert to pytorch datasets\n",
    "\n",
    "X = torch.FloatTensor(noised_signals)\n",
    "y = torch.FloatTensor(amplitudes)\n",
    "\n",
    "\n",
    "data = TensorDataset(X,y)\n",
    "\n",
    "train_data, val_data, test_data = random_split(data, lengths=[0.7,0.1,0.2])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e700172",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Most suggestions ive seen involve LSTM, probably worth looking into that more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d982d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This initial implimentation is all from claude AI\n",
    "# Going to try and get this working a bit and understand it\n",
    "# And will then update\n",
    "\n",
    "class ParameterPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=1, \n",
    "                           hidden_size=64, \n",
    "                           num_layers=2, \n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Reshape input to [batch, sequence, features]\n",
    "        x = x.unsqueeze(-1)  # Add feature dimension: [batch, 4000, 1]\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Use last output for prediction\n",
    "        last_out = lstm_out[:, -1, :]\n",
    "        return self.fc(last_out).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057b2839",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "Wrote this last year for the DSML exam, might need to be modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca024b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, optimizer, loss_fcn, n_epochs, train_dloader, val_dloader, start_epoch = 0, patience = 3):\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    best_val_epoch = 0\n",
    "\n",
    "    for epoch in range(start_epoch, start_epoch + n_epochs):\n",
    "        model.train()\n",
    "        tloss, vloss = 0, 0\n",
    "        true_preds, counts = 0., 0\n",
    "        for X_train, y_train in tqdm(train_dloader, desc='Epoch {}, training'.format(epoch+1)):\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(X_train)\n",
    "            loss = loss_fcn(pred, y_train.flatten())            \n",
    "            tloss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            true_preds+= (pred.argmax(dim = -1)==y_train.flatten()).sum()\n",
    "            counts += len(y_train)\n",
    "\n",
    "        train_acc = true_preds/counts\n",
    "        train_accs.append(train_acc)\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            vtrue_preds, vcount = 0., 0\n",
    "            for X_valid, y_valid in tqdm(val_dloader, desc='Epoch {}, validation'.format(epoch+1)):\n",
    "                pred = model(X_valid)\n",
    "                loss = loss_fcn(pred, y_valid.flatten())\n",
    "                vloss += loss.item()\n",
    "                vtrue_preds += (pred.argmax(dim = -1)==y_valid.flatten()).sum()\n",
    "                vcount += len(y_valid)\n",
    "        val_acc = vtrue_preds/vcount\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1:2d}] Training accuracy: {train_acc*100.0:05.2f}%, Validation accuracy: {val_acc*100.0:05.2f}%\")\n",
    "        train_losses.append(tloss/len(train_data))\n",
    "        val_losses.append(vloss/len(val_data))\n",
    "\n",
    "        if val_acc > max(val_accs[:-1], default=0):\n",
    "            print(\"New best validation performance\")\n",
    "            best_val_epoch = epoch\n",
    "\n",
    "        elif best_val_epoch <= epoch - patience:\n",
    "            print('No improvement in validation accuracy in last {} epochs'.format(patience))\n",
    "            break\n",
    "        \n",
    "\n",
    "        \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ebb4b9",
   "metadata": {},
   "source": [
    "# Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7f1a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ParameterPredictor()\n",
    "lossfcn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "trainModel(model, optimizer, lossfcn, 10, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b28a5d",
   "metadata": {},
   "source": [
    "random pycbc stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3189fd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycbc.waveform import get_td_waveform\n",
    "\n",
    "hp, hc = get_td_waveform(approximant=\"IMRPhenomD\", mass1=10,mass2=10,spin1z=0.9,delta_t=1/4096,f_lower=40)\n",
    "\n",
    "print(hp.sample_times)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
