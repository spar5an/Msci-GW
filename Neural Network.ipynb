{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c1ad3e6",
   "metadata": {},
   "source": [
    "# Basic Neural Network prediciting one parameter\n",
    "Now has been rewritten to use JHPY to make it easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8d0a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from JHPY import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3711a21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = generate_sine_data(data_points=1000, mean=10, std=2)\n",
    "\n",
    "train_loader = outputs[\"Train_Loader\"]\n",
    "val_loader = outputs[\"Val_Loader\"]\n",
    "test_loader = outputs[\"Test_Loader\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03fbbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'lstm_hidden_size': [128, 256],\n",
    "    'lstm_num_layers': [1],\n",
    "    'fc_layer_sizes': [[128, 64], [256, 128], [64, 32]],\n",
    "    'activation': ['relu'],\n",
    "    'dropout': [0.0],\n",
    "    'learning_rate': [0.01]\n",
    "}\n",
    "\n",
    "best_config, search_results = hyperparameter_search(\n",
    "    param_grid, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    n_epochs=50,\n",
    "    n_trials=None  # Try 10 random configurations (set to None to try all combinations)\n",
    ")\n",
    "\n",
    "print(\"\\nTop 5 configurations:\")\n",
    "for i, result in enumerate(search_results[:5]):\n",
    "    print(f\"\\n{i+1}. Val Loss: {result['best_val_loss']:.4f}, R²: {result['best_val_r2']:.4f}\")\n",
    "    print(f\"   Config: {result['config']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfef3a66",
   "metadata": {},
   "source": [
    "## Train Final Model with Best Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df8491e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with best configuration found from search\n",
    "final_model = ParameterPredictor(best_config)\n",
    "lossfcn = nn.MSELoss()\n",
    "\n",
    "# Extract learning rate from best_config or use default\n",
    "lr = best_config.get('learning_rate', 0.01)\n",
    "final_optimizer = torch.optim.Adam(final_model.parameters(), lr=lr)\n",
    "\n",
    "final_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    final_optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=4,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "\n",
    "# Train for more epochs since this is the final model\n",
    "final_outputs = train_model(\n",
    "    final_model, \n",
    "    final_optimizer, \n",
    "    lossfcn, \n",
    "    200,  # More epochs for final training\n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    patience=10,  # More patience for final model\n",
    "    scheduler=final_scheduler,\n",
    "    save_best_model=True,  # Save the best model checkpoint\n",
    "    model_path='best_final_model.pt'  # Path to save the model\n",
    ")\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(final_outputs['train_losses'], label='Train Loss')\n",
    "plt.plot(final_outputs['val_losses'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training History (Best Configuration)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest model saved at epoch {final_outputs['best_val_epoch'] + 1} with validation loss {final_outputs['best_val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933ba353",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7e9b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best saved model for evaluation\n",
    "eval_model, checkpoint = load_best_model('best_final_model.pt')\n",
    "\n",
    "# Display the configuration of the loaded model\n",
    "print(\"\\nModel Configuration:\")\n",
    "for key, value in checkpoint['model_config'].items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a007266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "labels = []\n",
    "rmse_list = []\n",
    "\n",
    "# Use eval_model (loaded from checkpoint)\n",
    "eval_model.eval()\n",
    "\n",
    "for X_test, y_test in test_loader:\n",
    "    test_predictions = eval_model(X_test).detach().numpy()\n",
    "    test_targets = y_test.numpy()\n",
    "    predictions.extend(test_predictions)\n",
    "    labels.extend(test_targets)\n",
    "    rmse = np.sqrt(np.mean((test_predictions - test_targets) ** 2))\n",
    "    rmse_list.append(rmse)\n",
    "\n",
    "# Calculate overall test metrics\n",
    "test_metrics = calculate_metrics(np.array(predictions), np.array(labels))\n",
    "\n",
    "print(\"Test Set Performance (Best Saved Model):\")\n",
    "print(f\"  RMSE: {test_metrics['rmse']:.4f}\")\n",
    "print(f\"  MAE: {test_metrics['mae']:.4f}\")\n",
    "print(f\"  R²: {test_metrics['r2']:.4f}\")\n",
    "\n",
    "# Plot prediction vs true distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "l_counts, l_edges = np.histogram(labels, bins=30, density=True)\n",
    "p_counts, p_edges = np.histogram(predictions, bins=l_edges, density=True)  # align bins with predictions\n",
    "plt.stairs(p_counts, p_edges, label='Predictions', color='C0')\n",
    "plt.stairs(l_counts, l_edges, label='True Labels', color='C1')\n",
    "plt.xlabel('Parameter value')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot scatter: predictions vs true values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(labels, predictions, alpha=0.5)\n",
    "plt.plot([min(labels), max(labels)], [min(labels), max(labels)], 'r--', label='Perfect prediction')\n",
    "plt.xlabel('True values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title(f'Predictions vs True Values (R²={test_metrics[\"r2\"]:.3f})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
