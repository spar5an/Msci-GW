{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyCBC Data Generator\n",
    "\n",
    "Functions to generate gravitational waveform data with detector projections for neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.11/site-packages/pycbc/types/array.py:36: UserWarning: Wswiglal-redir-stdio:\n",
      "\n",
      "SWIGLAL standard output/error redirection is enabled in IPython.\n",
      "This may lead to performance penalties. To disable locally, use:\n",
      "\n",
      "with lal.no_swig_redirect_standard_output_error():\n",
      "    ...\n",
      "\n",
      "To disable globally, use:\n",
      "\n",
      "lal.swig_redirect_standard_output_error(False)\n",
      "\n",
      "Note however that this will likely lead to error messages from\n",
      "LAL functions being either misdirected or lost when called from\n",
      "Jupyter notebooks.\n",
      "\n",
      "To suppress this warning, use:\n",
      "\n",
      "import warnings\n",
      "warnings.filterwarnings(\"ignore\", \"Wswiglal-redir-stdio\")\n",
      "import lal\n",
      "\n",
      "  import lal as _lal\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from pycbc.waveform import get_td_waveform\n",
    "from pycbc.detector import Detector\n",
    "from pycbc.types import TimeSeries\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "from typing import Dict, Callable, List, Tuple, Union\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate_config(config: Dict[str, Callable]) -> None:\n",
    "    \"\"\"Validate configuration dictionary.\"\"\"\n",
    "    if not isinstance(config, dict):\n",
    "        raise TypeError(\"config must be a dictionary\")\n",
    "    \n",
    "    if not config:\n",
    "        raise ValueError(\"config cannot be empty\")\n",
    "    \n",
    "    for key, value in config.items():\n",
    "        if not callable(value):\n",
    "            raise TypeError(f\"config['{key}'] must be callable (e.g., a distribution function)\")\n",
    "        \n",
    "        try:\n",
    "            test = value(size=2)\n",
    "            if not isinstance(test, np.ndarray):\n",
    "                raise TypeError(f\"config['{key}']() must return numpy array\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"config['{key}'] failed test call: {e}\")\n",
    "\n",
    "\n",
    "def _generate_parameter_sets(config: Dict[str, Callable], num_samples: int) -> List[Dict]:\n",
    "    \"\"\"Generate all parameter combinations upfront.\"\"\"\n",
    "    param_arrays = {}\n",
    "    for param_name, dist_func in config.items():\n",
    "        param_arrays[param_name] = dist_func(size=num_samples)\n",
    "    \n",
    "    param_dicts = []\n",
    "    for i in range(num_samples):\n",
    "        param_dict = {name: float(values[i]) for name, values in param_arrays.items()}\n",
    "        param_dicts.append(param_dict)\n",
    "    \n",
    "    return param_dicts\n",
    "\n",
    "\n",
    "def _generate_single_waveform(params: Dict, time_resolution: float, approximant: str,\n",
    "                              f_lower: float, detectors: List[str]) -> Dict:\n",
    "    \"\"\"Worker function to generate a single waveform and project to detectors.\"\"\"\n",
    "    try:\n",
    "        hp, hc = get_td_waveform(\n",
    "            approximant=approximant,\n",
    "            mass1=params['mass1'],\n",
    "            mass2=params['mass2'],\n",
    "            spin1z=params.get('spin1z', 0.0),\n",
    "            spin2z=params.get('spin2z', 0.0),\n",
    "            inclination=params.get('inclination', 0.0),\n",
    "            coa_phase=params.get('coa_phase', 0.0),\n",
    "            delta_t=time_resolution,\n",
    "            f_lower=f_lower\n",
    "        )\n",
    "        \n",
    "        # Get sky location parameters (defaults to north pole and zero polarization)\n",
    "        ra = params.get('ra', 0.0)\n",
    "        dec = params.get('dec', np.pi/2)  # North pole\n",
    "        polarization = params.get('polarization', 0.0)\n",
    "        tc = params.get('tc', 0.0)\n",
    "        \n",
    "        # Project to detectors\n",
    "        detector_signals = {}\n",
    "        for det_name in detectors:\n",
    "            detector = Detector(det_name)\n",
    "            signal = detector.project_wave(hp, hc, ra, dec, polarization, method='lal')\n",
    "            detector_signals[det_name] = signal\n",
    "        \n",
    "        result = {\n",
    "            'success': True,\n",
    "            'detectors': detector_signals,\n",
    "            'params': params\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {'success': False, 'error': str(e), 'params': params}\n",
    "\n",
    "\n",
    "def _generate_waveforms_parallel(param_dicts: List[Dict],\n",
    "                                time_resolution: float,\n",
    "                                approximant: str,\n",
    "                                f_lower: float,\n",
    "                                num_workers: int,\n",
    "                                show_progress: bool,\n",
    "                                detectors: List[str]) -> List[Dict]:\n",
    "    \"\"\"Generate waveforms in parallel using multiprocessing.\"\"\"\n",
    "    worker_func = partial(_generate_single_waveform,\n",
    "                          time_resolution=time_resolution,\n",
    "                          approximant=approximant,\n",
    "                          f_lower=f_lower,\n",
    "                          detectors=detectors)\n",
    "    \n",
    "    with Pool(processes=num_workers) as pool:\n",
    "        if show_progress:\n",
    "            results = list(tqdm(\n",
    "                pool.imap_unordered(worker_func, param_dicts, chunksize=100),\n",
    "                total=len(param_dicts),\n",
    "                desc=\"Generating waveforms\"\n",
    "            ))\n",
    "        else:\n",
    "            results = list(pool.imap_unordered(worker_func, param_dicts, chunksize=100))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def pycbc_data_generator(config: Dict[str, Callable],\n",
    "                        num_samples: int,\n",
    "                        time_resolution: float = 1/4096,\n",
    "                        approximant: str = 'IMRPhenomXP',\n",
    "                        f_lower: float = 40.0,\n",
    "                        num_workers: int = None,\n",
    "                        batch_size: int = 256,\n",
    "                        chunk_size: int = 10000,\n",
    "                        target_length: int = None,\n",
    "                        allow_padding: bool = False,\n",
    "                        train_split: float = 0.8,\n",
    "                        val_split: float = 0.1,\n",
    "                        show_progress: bool = True,\n",
    "                        detectors: List[str] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Generate PyCBC waveforms projected to detectors.\n",
    "    Returns PyTorch DataLoaders for training, validation, and testing.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    config : dict\n",
    "        Dictionary mapping parameter names to numpy distribution functions.\n",
    "        \n",
    "        Required parameters:\n",
    "        - 'mass1': Primary mass (solar masses)\n",
    "        - 'mass2': Secondary mass (solar masses)\n",
    "        \n",
    "        Optional parameters:\n",
    "        - 'spin1z', 'spin2z': Spin components\n",
    "        - 'inclination', 'coa_phase': Orientation angles\n",
    "        - 'ra': Right ascension (radians) - default: 0.0\n",
    "        - 'dec': Declination (radians) - default: π/2 (north pole)\n",
    "        - 'polarization': Polarization angle (radians) - default: 0.0\n",
    "        - 'tc': Coalescence time - default: 0.0\n",
    "        \n",
    "    num_samples : int\n",
    "        Total number of waveforms to generate\n",
    "    time_resolution : float\n",
    "        Time step (delta_t). Default: 1/4096\n",
    "    approximant : str\n",
    "        Waveform approximant. Default: 'IMRPhenomXP'\n",
    "    f_lower : float\n",
    "        Lower frequency cutoff (Hz). Default: 40.0\n",
    "    num_workers : int\n",
    "        Parallel processes. Default: min(cpu_count(), 8)\n",
    "    batch_size : int\n",
    "        DataLoader batch size. Default: 256\n",
    "    chunk_size : int\n",
    "        Process in chunks for memory. Default: 10000\n",
    "    target_length : int\n",
    "        Target length for padding/truncation. If None, uses max length.\n",
    "        Only used if allow_padding=True.\n",
    "    allow_padding : bool\n",
    "        Allow zero-padding of waveforms to common length. Default: False.\n",
    "        If False and waveforms have different lengths, raises ValueError.\n",
    "        If True, pads shorter waveforms with zeros at the beginning.\n",
    "    train_split : float\n",
    "        Training fraction. Default: 0.8\n",
    "    val_split : float\n",
    "        Validation fraction. Default: 0.1\n",
    "    detectors : list of str\n",
    "        Detector names. Default: ['H1', 'L1']\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict with 'train_loader', 'val_loader', 'test_loader', 'metadata'\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    If ra, dec, or polarization are not provided in config, they default to:\n",
    "    - ra = 0.0\n",
    "    - dec = π/2 (north pole)\n",
    "    - polarization = 0.0\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    _validate_config(config)\n",
    "    if num_samples <= 0:\n",
    "        raise ValueError(\"num_samples must be positive\")\n",
    "    if not 0 < train_split < 1 or not 0 < val_split < 1:\n",
    "        raise ValueError(\"train_split and val_split must be between 0 and 1\")\n",
    "    if train_split + val_split >= 1:\n",
    "        raise ValueError(\"train_split + val_split must be < 1\")\n",
    "    \n",
    "    if num_workers is None:\n",
    "        num_workers = min(cpu_count(), 8)\n",
    "    \n",
    "    # Set default detectors\n",
    "    if detectors is None:\n",
    "        detectors = ['H1', 'L1']\n",
    "    \n",
    "    # Check which sky parameters are provided\n",
    "    sky_params_provided = {\n",
    "        'ra': 'ra' in config,\n",
    "        'dec': 'dec' in config,\n",
    "        'polarization': 'polarization' in config\n",
    "    }\n",
    "    \n",
    "    if any(sky_params_provided.values()):\n",
    "        print(f\"Generating {num_samples} waveforms with projection to {detectors}\")\n",
    "        print(f\"  Sky parameters: ra={'provided' if sky_params_provided['ra'] else 'default (0.0)'}, \"\n",
    "              f\"dec={'provided' if sky_params_provided['dec'] else 'default (π/2)'}, \"\n",
    "              f\"psi={'provided' if sky_params_provided['polarization'] else 'default (0.0)'}\")\n",
    "    else:\n",
    "        print(f\"Generating {num_samples} waveforms with projection to {detectors}\")\n",
    "        print(f\"  Using default sky location: ra=0.0, dec=π/2 (north pole), psi=0.0\")\n",
    "    \n",
    "    # Generate all parameters upfront\n",
    "    param_dicts = _generate_parameter_sets(config, num_samples)\n",
    "    \n",
    "    # Process in chunks for memory efficiency\n",
    "    all_successful = []\n",
    "    all_failed = []\n",
    "    num_chunks = (num_samples + chunk_size - 1) // chunk_size\n",
    "    \n",
    "    for chunk_idx in range(num_chunks):\n",
    "        chunk_start = chunk_idx * chunk_size\n",
    "        chunk_end = min(chunk_start + chunk_size, num_samples)\n",
    "        chunk_params = param_dicts[chunk_start:chunk_end]\n",
    "        \n",
    "        if num_chunks > 1:\n",
    "            print(f\"\\nChunk {chunk_idx + 1}/{num_chunks} ({len(chunk_params)} waveforms)...\")\n",
    "        \n",
    "        # Generate waveforms with detector projection\n",
    "        chunk_results = _generate_waveforms_parallel(\n",
    "            chunk_params, time_resolution, approximant, f_lower, num_workers, show_progress, detectors\n",
    "        )\n",
    "        \n",
    "        # Single pass: separate and accumulate\n",
    "        for r in chunk_results:\n",
    "            if r['success']:\n",
    "                all_successful.append(r)\n",
    "            else:\n",
    "                all_failed.append(r)\n",
    "        \n",
    "        if num_chunks > 1:\n",
    "            print(f\"  Chunk: {len([r for r in chunk_results if r['success']])} successful\")\n",
    "    \n",
    "    if not all_successful:\n",
    "        raise RuntimeError(\"No waveforms were successfully generated!\")\n",
    "    \n",
    "    num_success = len(all_successful)\n",
    "    num_failed = len(all_failed)\n",
    "    print(f\"\\nGeneration complete: {num_success} successful, {num_failed} failed\")\n",
    "    \n",
    "    # Extract everything in one pass with pre-allocated arrays\n",
    "    print(f\"\\nProcessing {num_success} waveforms...\")\n",
    "    \n",
    "    # Get parameter names and detector names\n",
    "    param_names = list(all_successful[0]['params'].keys())\n",
    "    num_params = len(param_names)\n",
    "    detector_names = list(all_successful[0]['detectors'].keys())\n",
    "    num_detectors = len(detector_names)\n",
    "    \n",
    "    print(f\"  Detector channels: {detector_names}\")\n",
    "    \n",
    "    # Pre-scan for lengths\n",
    "    lengths = np.array([len(w['detectors'][detector_names[0]]) for w in all_successful])\n",
    "    min_len, max_len = lengths.min(), lengths.max()\n",
    "    \n",
    "    print(f\"  Waveform lengths: min={min_len}, max={max_len}\")\n",
    "    \n",
    "    # Check if padding is needed\n",
    "    if min_len != max_len:\n",
    "        if not allow_padding:\n",
    "            raise ValueError(\n",
    "                f\"Waveforms have variable lengths (min={min_len}, max={max_len}). \"\n",
    "                f\"This happens because PyCBC generates waveforms of different durations \"\n",
    "                f\"depending on the binary parameters (especially masses). \"\n",
    "                f\"\\n\\nTo handle this, set allow_padding=True to zero-pad shorter waveforms. \"\n",
    "                f\"Padding adds zeros at the beginning (before the signal), which is \"\n",
    "                f\"physically meaningful and standard practice in GW analysis.\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"  WARNING: Variable lengths detected. Applying zero-padding.\")\n",
    "            print(f\"  Padding will be added at the beginning (before signal starts).\")\n",
    "    \n",
    "    # Determine target length\n",
    "    if target_length is None:\n",
    "        target_length = max_len\n",
    "    elif target_length < max_len:\n",
    "        print(f\"  WARNING: target_length ({target_length}) < max length ({max_len}). Will truncate!\")\n",
    "    \n",
    "    print(f\"  Target length: {target_length}\")\n",
    "    \n",
    "    # Pre-allocate arrays\n",
    "    signal_array = np.zeros((num_success, num_detectors, target_length), dtype=np.float32)\n",
    "    param_array = np.zeros((num_success, num_params), dtype=np.float32)\n",
    "    \n",
    "    # Single pass: extract everything\n",
    "    for i, w in enumerate(all_successful):\n",
    "        # Extract detector signals\n",
    "        for j, det_name in enumerate(detector_names):\n",
    "            signal_data = w['detectors'][det_name].numpy()\n",
    "            current_len = len(signal_data)\n",
    "            \n",
    "            # Pad or truncate\n",
    "            if current_len <= target_length:\n",
    "                # Pad at beginning (zero-padding before signal starts)\n",
    "                start_idx = target_length - current_len\n",
    "                signal_array[i, j, start_idx:] = signal_data\n",
    "            else:\n",
    "                # Truncate (keep end where merger happens)\n",
    "                signal_array[i, j] = signal_data[-target_length:]\n",
    "        \n",
    "        # Extract parameters\n",
    "        for j, name in enumerate(param_names):\n",
    "            param_array[i, j] = w['params'][name]\n",
    "    \n",
    "    print(f\"  Converting to PyTorch tensors...\")\n",
    "    \n",
    "    # Convert to PyTorch\n",
    "    X = torch.from_numpy(signal_array)  # (N, num_detectors, T)\n",
    "    y = torch.from_numpy(param_array)    # (N, num_params)\n",
    "    \n",
    "    print(f\"  Tensors: X={X.shape}, y={y.shape}\")\n",
    "    \n",
    "    # Create dataset and split\n",
    "    dataset = TensorDataset(X, y)\n",
    "    total_size = len(dataset)\n",
    "    train_size = int(train_split * total_size)\n",
    "    val_size = int(val_split * total_size)\n",
    "    test_size = total_size - train_size - val_size\n",
    "    \n",
    "    train_data, val_data, test_data = random_split(\n",
    "        dataset, [train_size, val_size, test_size]\n",
    "    )\n",
    "    \n",
    "    print(f\"  Splits: train={train_size}, val={val_size}, test={test_size}\")\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f\"\\nReady! DataLoaders with batch_size={batch_size}\")\n",
    "    \n",
    "    return {\n",
    "        'train_loader': train_loader,\n",
    "        'val_loader': val_loader,\n",
    "        'test_loader': test_loader,\n",
    "        'metadata': {\n",
    "            'parameter_names': param_names,\n",
    "            'num_samples': num_success,\n",
    "            'num_failed': num_failed,\n",
    "            'waveform_shape': tuple(X.shape[1:]),\n",
    "            'channels': detector_names,\n",
    "            'train_size': train_size,\n",
    "            'val_size': val_size,\n",
    "            'test_size': test_size,\n",
    "            'batch_size': batch_size,\n",
    "            'time_resolution': time_resolution,\n",
    "            'approximant': approximant,\n",
    "            'f_lower': f_lower,\n",
    "            'detectors': detector_names,\n",
    "            'target_length': target_length,\n",
    "            'original_length_range': (int(min_len), int(max_len)),\n",
    "            'chunk_size': chunk_size,\n",
    "            'sky_params_provided': sky_params_provided,\n",
    "            'allow_padding': allow_padding\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataloaders(result: Dict, save_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Save the datasets from a pycbc_data_generator result.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    result : dict\n",
    "        The result dictionary from pycbc_data_generator containing \n",
    "        train_loader, val_loader, test_loader, and metadata\n",
    "    save_path : str\n",
    "        Path where to save the data (e.g., 'my_data.pt')\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> result = pycbc_data_generator(config, num_samples=1000)\n",
    "    >>> save_dataloaders(result, 'my_waveforms.pt')\n",
    "    \"\"\"\n",
    "    print(f\"Saving datasets to {save_path}...\")\n",
    "    \n",
    "    # Extract the underlying datasets and indices from the DataLoaders\n",
    "    train_dataset = result['train_loader'].dataset\n",
    "    val_dataset = result['val_loader'].dataset\n",
    "    test_dataset = result['test_loader'].dataset\n",
    "    \n",
    "    # Get the full tensors from the base dataset\n",
    "    # The subsets have .dataset (base) and .indices attributes\n",
    "    base_dataset = train_dataset.dataset\n",
    "    X = base_dataset.tensors[0]\n",
    "    y = base_dataset.tensors[1]\n",
    "    \n",
    "    save_data = {\n",
    "        'train_indices': train_dataset.indices,\n",
    "        'val_indices': val_dataset.indices,\n",
    "        'test_indices': test_dataset.indices,\n",
    "        'X': X,\n",
    "        'y': y,\n",
    "        'metadata': result['metadata']\n",
    "    }\n",
    "    \n",
    "    torch.save(save_data, save_path)\n",
    "    print(f\"  Saved successfully!\")\n",
    "\n",
    "\n",
    "def load_dataloaders(load_path: str, batch_size: int = None, shuffle_train: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Load previously saved datasets and create DataLoaders.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    load_path : str\n",
    "        Path to the saved data file (created with save_dataloaders)\n",
    "    batch_size : int, optional\n",
    "        Batch size for DataLoaders. If None, uses the batch_size from metadata.\n",
    "    shuffle_train : bool\n",
    "        Whether to shuffle training data. Default: True\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict with 'train_loader', 'val_loader', 'test_loader', 'metadata'\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> # Save after generation\n",
    "    >>> result = pycbc_data_generator(config, num_samples=1000)\n",
    "    >>> save_dataloaders(result, 'my_data.pt')\n",
    "    >>> \n",
    "    >>> # Later, load the data\n",
    "    >>> loaded = load_dataloaders('my_data.pt')\n",
    "    >>> train_loader = loaded['train_loader']\n",
    "    >>> val_loader = loaded['val_loader']\n",
    "    >>> test_loader = loaded['test_loader']\n",
    "    \"\"\"\n",
    "    print(f\"Loading datasets from {load_path}...\")\n",
    "    \n",
    "    # Load the saved data\n",
    "    save_data = torch.load(load_path, weights_only=False)\n",
    "    \n",
    "    X = save_data['X']\n",
    "    y = save_data['y']\n",
    "    train_indices = save_data['train_indices']\n",
    "    val_indices = save_data['val_indices']\n",
    "    test_indices = save_data['test_indices']\n",
    "    metadata = save_data['metadata']\n",
    "    \n",
    "    # Use saved batch_size if not provided\n",
    "    if batch_size is None:\n",
    "        batch_size = metadata['batch_size']\n",
    "    else:\n",
    "        # Update metadata with new batch_size\n",
    "        metadata = metadata.copy()\n",
    "        metadata['batch_size'] = batch_size\n",
    "    \n",
    "    print(f\"  Tensors: X={X.shape}, y={y.shape}\")\n",
    "    print(f\"  Splits: train={len(train_indices)}, val={len(val_indices)}, test={len(test_indices)}\")\n",
    "    \n",
    "    # Recreate the dataset\n",
    "    full_dataset = TensorDataset(X, y)\n",
    "    \n",
    "    # Create subsets using the saved indices\n",
    "    from torch.utils.data import Subset\n",
    "    train_data = Subset(full_dataset, train_indices)\n",
    "    val_data = Subset(full_dataset, val_indices)\n",
    "    test_data = Subset(full_dataset, test_indices)\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=shuffle_train)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f\"\\nReady! DataLoaders with batch_size={batch_size}\")\n",
    "    \n",
    "    return {\n",
    "        'train_loader': train_loader,\n",
    "        'val_loader': val_loader,\n",
    "        'test_loader': test_loader,\n",
    "        'metadata': metadata\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure parameter distributions\n",
    "config = {\n",
    "    # Required parameters\n",
    "    'mass1': lambda size: np.random.uniform(10, 50, size=size),\n",
    "    'mass2': lambda size: np.random.uniform(10, 50, size=size),\n",
    "    \n",
    "    # Spin parameters\n",
    "    'spin1z': lambda size: np.random.uniform(-0.9, 0.9, size=size),\n",
    "    'spin2z': lambda size: np.random.uniform(-0.9, 0.9, size=size),\n",
    "    \n",
    "    # Orientation parameters\n",
    "    'inclination': lambda size: np.random.uniform(0, np.pi, size=size),\n",
    "    'coa_phase': lambda size: np.random.uniform(0, 2*np.pi, size=size),\n",
    "    \n",
    "    # Sky location parameters (for realistic detector responses)\n",
    "    'ra': lambda size: np.random.uniform(0, 2*np.pi, size=size),\n",
    "    'dec': lambda size: np.random.uniform(-np.pi/2, np.pi/2, size=size),\n",
    "    'polarization': lambda size: np.random.uniform(0, 2*np.pi, size=size),\n",
    "}\n",
    "\n",
    "# Generate data with H1, L1, and V1 detectors\n",
    "result = pycbc_data_generator(\n",
    "    config, \n",
    "    num_samples=200,\n",
    "    batch_size=32,\n",
    "    num_workers=4,\n",
    "    detectors=['H1', 'L1', 'V1'],  # Three detector network\n",
    "    allow_padding=True,  # Handle variable-length waveforms\n",
    "    train_split=0.7,\n",
    "    val_split=0.15,\n",
    "    # test_split will be 0.15 (remainder)\n",
    ")\n",
    "\n",
    "# Extract components\n",
    "train_loader = result['train_loader']\n",
    "val_loader = result['val_loader']\n",
    "test_loader = result['test_loader']\n",
    "metadata = result['metadata']\n",
    "\n",
    "# Display metadata\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATASET METADATA\")\n",
    "print(\"=\"*60)\n",
    "for key, value in metadata.items():\n",
    "    print(f\"{key:25s}: {value}\")\n",
    "\n",
    "# Show example batch from each split\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXAMPLE BATCHES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, loader in [('TRAIN', train_loader), ('VAL', val_loader), ('TEST', test_loader)]:\n",
    "    for waveforms, params in loader:\n",
    "        print(f\"\\n{name} Batch:\")\n",
    "        print(f\"  Waveforms shape: {waveforms.shape}  # (batch, detectors, time)\")\n",
    "        print(f\"  Parameters shape: {params.shape}  # (batch, num_params)\")\n",
    "        print(f\"  Detector channels: {metadata['channels']}\")\n",
    "        print(f\"  First sample - H1[0:5]: {waveforms[0, 0, :5]}\")\n",
    "        print(f\"  First sample - L1[0:5]: {waveforms[0, 1, :5]}\")\n",
    "        print(f\"  First sample - V1[0:5]: {waveforms[0, 2, :5]}\")\n",
    "        print(f\"  First sample params: {params[0]}\")\n",
    "        break\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING & LOADING DEMO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save the dataloaders\n",
    "save_dataloaders(result, 'test_waveforms.pt')\n",
    "\n",
    "# Load them back\n",
    "loaded_result = load_dataloaders('test_waveforms.pt', batch_size=64)  # Changed batch size\n",
    "\n",
    "print(\"\\nLoaded with different batch size!\")\n",
    "print(f\"  Original batch size: {metadata['batch_size']}\")\n",
    "print(f\"  New batch size: {loaded_result['metadata']['batch_size']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Ready for training!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 200000 waveforms with projection to ['H1', 'L1']\n",
      "  Using default sky location: ra=0.0, dec=π/2 (north pole), psi=0.0\n",
      "\n",
      "Chunk 1/20 (10000 waveforms)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating waveforms: 100%|██████████| 10000/10000 [06:11<00:00, 26.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Chunk: 10000 successful\n",
      "\n",
      "Chunk 2/20 (10000 waveforms)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating waveforms: 100%|██████████| 10000/10000 [07:25<00:00, 22.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Chunk: 10000 successful\n",
      "\n",
      "Chunk 3/20 (10000 waveforms)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating waveforms: 100%|██████████| 10000/10000 [06:21<00:00, 26.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Chunk: 10000 successful\n",
      "\n",
      "Chunk 4/20 (10000 waveforms)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating waveforms: 100%|██████████| 10000/10000 [06:33<00:00, 25.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Chunk: 10000 successful\n",
      "\n",
      "Chunk 5/20 (10000 waveforms)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating waveforms: 100%|██████████| 10000/10000 [06:24<00:00, 26.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Chunk: 10000 successful\n",
      "\n",
      "Chunk 6/20 (10000 waveforms)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating waveforms: 100%|██████████| 10000/10000 [06:22<00:00, 26.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Chunk: 10000 successful\n",
      "\n",
      "Chunk 7/20 (10000 waveforms)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating waveforms: 100%|██████████| 10000/10000 [06:38<00:00, 25.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Chunk: 10000 successful\n",
      "\n",
      "Chunk 8/20 (10000 waveforms)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating waveforms: 100%|██████████| 10000/10000 [06:21<00:00, 26.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Chunk: 10000 successful\n",
      "\n",
      "Chunk 9/20 (10000 waveforms)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating waveforms: 100%|██████████| 10000/10000 [06:25<00:00, 25.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Chunk: 10000 successful\n",
      "\n",
      "Chunk 10/20 (10000 waveforms)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating waveforms: 100%|██████████| 10000/10000 [06:25<00:00, 25.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Chunk: 10000 successful\n",
      "\n",
      "Chunk 11/20 (10000 waveforms)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating waveforms: 100%|██████████| 10000/10000 [06:13<00:00, 26.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Chunk: 10000 successful\n",
      "\n",
      "Chunk 12/20 (10000 waveforms)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating waveforms: 100%|██████████| 10000/10000 [06:13<00:00, 26.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Chunk: 10000 successful\n",
      "\n",
      "Chunk 13/20 (10000 waveforms)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating waveforms: 100%|██████████| 10000/10000 [06:11<00:00, 26.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Chunk: 10000 successful\n",
      "\n",
      "Chunk 14/20 (10000 waveforms)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating waveforms: 100%|██████████| 10000/10000 [06:10<00:00, 26.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Chunk: 10000 successful\n",
      "\n",
      "Chunk 15/20 (10000 waveforms)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating waveforms: 100%|██████████| 10000/10000 [06:04<00:00, 27.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Chunk: 10000 successful\n",
      "\n",
      "Chunk 16/20 (10000 waveforms)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating waveforms: 100%|██████████| 10000/10000 [06:02<00:00, 27.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Chunk: 10000 successful\n",
      "\n",
      "Chunk 17/20 (10000 waveforms)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating waveforms: 100%|██████████| 10000/10000 [05:59<00:00, 27.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Chunk: 10000 successful\n",
      "\n",
      "Chunk 18/20 (10000 waveforms)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating waveforms: 100%|██████████| 10000/10000 [05:57<00:00, 27.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Chunk: 10000 successful\n",
      "\n",
      "Chunk 19/20 (10000 waveforms)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating waveforms: 100%|██████████| 10000/10000 [05:56<00:00, 28.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Chunk: 10000 successful\n",
      "\n",
      "Chunk 20/20 (10000 waveforms)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating waveforms:   0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Configure parameter distributions\n",
    "config = {\n",
    "    # Required parameters\n",
    "    'mass1': lambda size: np.random.uniform(10, 50, size=size),\n",
    "    'mass2': lambda size: np.random.uniform(10, 50, size=size),\n",
    "}\n",
    "\n",
    "# Generate data with H1, L1, and V1 detectors\n",
    "result = pycbc_data_generator(\n",
    "    config, \n",
    "    num_samples=200000,\n",
    "    batch_size=2048,\n",
    "    num_workers=36,\n",
    "    detectors=['H1', 'L1'],  # Two detector network\n",
    "    allow_padding=False,  # Handle variable-length waveforms\n",
    "    train_split=0.8,\n",
    "    val_split=0.1,\n",
    "    # test_split will be 0.1 (remainder)\n",
    ")\n",
    "\n",
    "# Extract components\n",
    "train_loader = result['train_loader']\n",
    "val_loader = result['val_loader']\n",
    "test_loader = result['test_loader']\n",
    "metadata = result['metadata']\n",
    "\n",
    "# Display metadata\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATASET METADATA\")\n",
    "print(\"=\"*60)\n",
    "for key, value in metadata.items():\n",
    "    print(f\"{key:25s}: {value}\")\n",
    "\n",
    "# Show example batch from each split\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXAMPLE BATCHES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, loader in [('TRAIN', train_loader), ('VAL', val_loader), ('TEST', test_loader)]:\n",
    "    for waveforms, params in loader:\n",
    "        print(f\"\\n{name} Batch:\")\n",
    "        print(f\"  Waveforms shape: {waveforms.shape}  # (batch, detectors, time)\")\n",
    "        print(f\"  Parameters shape: {params.shape}  # (batch, num_params)\")\n",
    "        print(f\"  Detector channels: {metadata['channels']}\")\n",
    "        print(f\"  First sample - H1[0:5]: {waveforms[0, 0, :5]}\")\n",
    "        print(f\"  First sample - L1[0:5]: {waveforms[0, 1, :5]}\")\n",
    "        print(f\"  First sample params: {params[0]}\")\n",
    "        plt.plot(waveforms[0, 0, :].numpy(), label='H1')\n",
    "        plt.plot(waveforms[0, 1, :].numpy(), label='L1')\n",
    "        break\n",
    "    break\n",
    "\n",
    "save_dataloaders(result, \"data_loaders.pt\")\n",
    "\n",
    "result_loaded = load_dataloaders(\"data_loaders.pt\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Ready for training!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
